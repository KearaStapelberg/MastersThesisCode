{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pprint, time\n",
    "\n",
    "import nltk\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "from nltk.corpus import treebank\n",
    "\n",
    "\n",
    "from nltk.classify import MaxentClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "import pycrfsuite\n",
    "from nltk.tag import hmm\n",
    "from nltk.classify import megam\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from nltk.tag import BrillTaggerTrainer\n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.tag import DefaultTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\kstap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\kstap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#download the treebank corpus from nltk\n",
    "nltk.download('treebank')\n",
    " \n",
    "#download the universal tagset from nltk\n",
    "nltk.download('universal_tagset')\n",
    " \n",
    "#reading the Treebank tagged sentences\n",
    "tagged_sentences = list(nltk.corpus.treebank.tagged_sents(tagset='universal'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corruption function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrupt_training_set(corpus, switch_prob):\n",
    "    \"\"\"\n",
    "    Corrupt a tagged corpus by randomly switching POS tags with a given probability.\n",
    "\n",
    "    Args:\n",
    "    corpus (list): A list of tagged sentences.\n",
    "    switch_prob (float): The probability of switching out a POS tag for corruption.\n",
    "\n",
    "    Returns:\n",
    "    list: The corrupted tagged corpus.\n",
    "    int: The number of errors induced.\n",
    "    \"\"\"\n",
    "    corrupted_corpus = []\n",
    "    error_count = 0\n",
    "\n",
    "    for sentence in corpus:\n",
    "        corrupted_sentence = []\n",
    "        for word, pos_tag in sentence:\n",
    "            if random.random() < switch_prob:\n",
    "                # Randomly choose a new POS tag (ensure it's different from the original)\n",
    "                new_pos_tag = pos_tag\n",
    "                while new_pos_tag == pos_tag:\n",
    "                    new_pos_tag = random.choice(['NOUN', 'VERB', 'ADJ', 'ADV', 'PRON', 'DET', 'NUM', 'CONJ', 'PRT', '.', 'X'])\n",
    "                corrupted_sentence.append((word, new_pos_tag))\n",
    "                error_count += 1\n",
    "            else:\n",
    "                corrupted_sentence.append((word, pos_tag))\n",
    "        corrupted_corpus.append(corrupted_sentence)\n",
    "\n",
    "    return corrupted_corpus, error_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature function and feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_features(sentence, i):\n",
    "\n",
    "    \"\"\"\n",
    "    Extract features for a given index in a sentence.\n",
    "\n",
    "    Parameters:\n",
    "    - sentence: List of feature-label pairs.\n",
    "    - i: index\n",
    "\n",
    "    Returns:\n",
    "    - features: a dictionary of features on a given index.\n",
    "    \"\"\"\n",
    "    word = sentence[i][0]\n",
    "    tag = sentence[i][1]\n",
    "    features = {\n",
    "        'word': word,\n",
    "        'is_first': i == 0,  # if the word is the first word\n",
    "        'is_last': i == len(sentence) - 1,  # if the word is the last word\n",
    "        'is_capitalized': word[0].upper() == word[0],\n",
    "        'is_all_caps': word.upper() == word,  # word is in uppercase\n",
    "        'is_all_lower': word.lower() == word,  # word is in lowercase\n",
    "        # prefix of the word\n",
    "        'prefix-1': word[0],\n",
    "        'prefix-2': word[:2],\n",
    "        'prefix-3': word[:3],\n",
    "        # suffix of the word\n",
    "        'suffix-1': word[-1],\n",
    "        'suffix-2': word[-2:],\n",
    "        'suffix-3': word[-3:],\n",
    "        # extracting previous word\n",
    "        'prev_word': '' if i == 0 else sentence[i - 1][0],\n",
    "        # extracting next word\n",
    "        'next_word': '' if i == len(sentence) - 1 else sentence[i + 1][0],\n",
    "        'has_hyphen': '-' in word,  # if word has a hyphen\n",
    "        'is_numeric': word.isdigit(),  # if word is numeric\n",
    "        'capitals_inside': word[1:].lower() != word[1:]\n",
    "    }\n",
    "\n",
    "    # Add previous tag and its previous tag\n",
    "    prev_tag = '' if i == 0 else sentence[i - 1][1]\n",
    "    prev_prev_tag = '' if i < 2 else sentence[i - 2][1]\n",
    "    features['prev_prev_tag'] = f'{prev_prev_tag}_{prev_tag}'\n",
    "\n",
    "    # Add word after the next word\n",
    "    features['next_next_word'] = '' if i > len(sentence) - 3 else sentence[i + 2][0]\n",
    "\n",
    "    # Add word before the previous word\n",
    "    features['prev_prev_word'] = '' if i < 2 else sentence[i - 2][0]\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract CRF features\n",
    "X = []\n",
    "y = []\n",
    "for sentence in tagged_sentences:\n",
    "\tX_sentence = []\n",
    "\ty_sentence = []\n",
    "\tfor i in range(len(sentence)):\n",
    "\t\tX_sentence.append(word_features(sentence, i))\n",
    "\t\ty_sentence.append(sentence[i][1])\n",
    "\tX.append(X_sentence)\n",
    "\ty.append(y_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation for MEMM and CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV tuning function\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def train_and_tune_memm(MEMM_train, max_iter_values, num_folds=5):\n",
    "    \"\"\"\n",
    "    Train and tune a Maximum Entropy Markov Model (MEMM) using cross-validation.\n",
    "\n",
    "    Parameters:\n",
    "    - MEMM_train: List of feature-label pairs for training.\n",
    "    - max_iter_values: List of max_iter values to tune.\n",
    "    - num_folds: Number of folds for cross-validation.\n",
    "\n",
    "    Returns:\n",
    "    - best_max_iter: The best max_iter value found.\n",
    "    - best_f1: The F1 score achieved with the best max_iter value.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize variables to keep track of the best max_iter and its associated F1 score\n",
    "    best_max_iter = None\n",
    "    best_f1 = 0.0\n",
    "\n",
    "    # Define the number of folds for cross-validation\n",
    "    kf = KFold(n_splits=num_folds)\n",
    "\n",
    "    for max_iter in max_iter_values:\n",
    "        f1_scores = []\n",
    "\n",
    "        for train_idx, valid_idx in kf.split(MEMM_train):\n",
    "            train_set = [MEMM_train[i] for i in train_idx]\n",
    "            valid_set = [MEMM_train[i] for i in valid_idx]\n",
    "\n",
    "            maxent_classifier = MaxentClassifier.train(train_set, algorithm='gis', max_iter=max_iter)\n",
    "\n",
    "            valid_features = [features for features, _ in valid_set]\n",
    "            valid_labels = [pos for _, pos in valid_set]\n",
    "\n",
    "            predictions = [maxent_classifier.classify(features) for features in valid_features]\n",
    "\n",
    "            f1 = f1_score(valid_labels, predictions, average='weighted')\n",
    "            f1_scores.append(f1)\n",
    "\n",
    "        # Calculate the average F1 score across folds\n",
    "        avg_f1 = np.mean(f1_scores)\n",
    "\n",
    "        # Check if this max_iter gives a better F1 score than the current best\n",
    "        if avg_f1 > best_f1:\n",
    "            best_max_iter = max_iter\n",
    "            best_f1 = avg_f1\n",
    "\n",
    "    return best_max_iter\n",
    "\n",
    "# Example usage:\n",
    "# best_max_iter, best_f1 = train_and_tune_memm(MEMM_train40, max_iter_values=[5, 10, 15, 20])\n",
    "# print(f\"Best max_iter: {best_max_iter}\")\n",
    "# print(f\"Best F1 Score: {best_f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and tune\n",
    "\n",
    "def train_and_tune_crf_with_cv(X, y, param_grid, n_folds=5):\n",
    "\n",
    "    \"\"\"\n",
    "    Train and tune a Conditional Random Fields (CRF) Model using cross-validation.\n",
    "\n",
    "    Parameters:\n",
    "    - X: list of extracted features\n",
    "    - y: list of corresponding tags\n",
    "    - param_grid: search grid dictionary.\n",
    "    - n_folds: Number of folds for cross-validation.\n",
    "\n",
    "    Returns:\n",
    "    - best_params: A dictionary of the best parameter values found.\n",
    "    \"\"\"\n",
    "        \n",
    "    best_f1 = 0.0\n",
    "    best_params = {}\n",
    "\n",
    "    for max_iter in param_grid['max_iterations']:\n",
    "        for c1 in param_grid['c1']:\n",
    "            for c2 in param_grid['c2']:\n",
    "                f1_scores = []\n",
    "\n",
    "                for fold in range(n_folds):\n",
    "                    # Split data into training and validation sets\n",
    "                    train_indices = [i for i in range(len(X)) if i % n_folds != fold]\n",
    "                    valid_indices = [i for i in range(len(X)) if i % n_folds == fold]\n",
    "\n",
    "                    X_train_fold = [X[i] for i in train_indices]\n",
    "                    y_train_fold = [y[i] for i in train_indices]\n",
    "                    X_valid_fold = [X[i] for i in valid_indices]\n",
    "                    y_valid_fold = [y[i] for i in valid_indices]\n",
    "\n",
    "                    # Train the CRF model\n",
    "                    trainer = pycrfsuite.Trainer(verbose=False)\n",
    "                    for x_train, y_train in zip(X_train_fold, y_train_fold):\n",
    "                        trainer.append(x_train, y_train)\n",
    "                    trainer.set_params({\n",
    "                        'max_iterations': max_iter,\n",
    "                        'c1': c1,\n",
    "                        'c2': c2,\n",
    "                        'feature.possible_transitions': True\n",
    "                    })\n",
    "                    trainer.train('temp_model.crfsuite')\n",
    "\n",
    "                    # Test the CRF model\n",
    "                    tagger = pycrfsuite.Tagger()\n",
    "                    tagger.open('temp_model.crfsuite')\n",
    "\n",
    "                    CRF_predictions = [tagger.tag(instance) for instance in X_valid_fold]\n",
    "\n",
    "                    CRF_flat_predictions = [tag for instance_tags in CRF_predictions for tag in instance_tags]\n",
    "                    CRF_flat_ground_truth = [tag for instance_tags in y_valid_fold for tag in instance_tags]\n",
    "\n",
    "                    f1 = f1_score(CRF_flat_ground_truth, CRF_flat_predictions, average='weighted')\n",
    "                    f1_scores.append(f1)\n",
    "\n",
    "                mean_f1 = np.mean(f1_scores)\n",
    "                if mean_f1 > best_f1:\n",
    "                    best_f1 = mean_f1\n",
    "                    best_params = {\n",
    "                        'max_iterations': max_iter,\n",
    "                        'c1': c1,\n",
    "                        'c2': c2,\n",
    "                        'feature.possible_transitions': True\n",
    "                    }\n",
    "\n",
    "    #print(\"Best Parameters: \", best_params)\n",
    "    #print(\"Best F1 Score: \", best_f1)\n",
    "\n",
    "    return best_params\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the parameter grid for tuning crf\n",
    "param_grid = {\n",
    "    'max_iterations': [20,50,100],\n",
    "    'c1': [0.01, 0.1, 1.0, 10],\n",
    "    'c2': [1e-4, 1e-3, 1e-2, 1e-1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation\n",
    "random.seed(1234)\n",
    "valid_set, model_set = train_test_split(tagged_sentences, train_size=0.1)\n",
    "\n",
    "# feature extraction    \n",
    "X_valid = []\n",
    "y_valid = []\n",
    "for sentence in valid_set:\n",
    "    X_sentence = []\n",
    "    y_sentence = []\n",
    "    for i in range(len(sentence)):\n",
    "        X_sentence.append(word_features(sentence, i))\n",
    "        y_sentence.append(sentence[i][1])\n",
    "    X_valid.append(X_sentence)\n",
    "    y_valid.append(y_sentence)    \n",
    "\n",
    "MEMM_valid = []  # Collect feature-label pairs for MEMM\n",
    "for sentence_features, sentence_labels in zip(X_valid, y_valid):\n",
    "    MEMM_valid.extend(list(zip(sentence_features, sentence_labels)))  \n",
    "\n",
    "best_max_iter = train_and_tune_memm(MEMM_valid, max_iter_values=[10,20,30])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corruption_counts(data, prob, num_repetitions):\n",
    "\n",
    "    tagged_words_true = [ tup for sent in data for tup in sent ]\n",
    "    true_labels = [pos for _, pos in tagged_words_true]\n",
    "    f1_scores = []\n",
    "    error_counts = []\n",
    "\n",
    "    for _ in range(num_repetitions):\n",
    "        corrupt_train, error_count = corrupt_training_set(data, switch_prob=prob)\n",
    "        error_counts.append(error_count)\n",
    "\n",
    "        tagged_words = [ tup for sent in corrupt_train for tup in sent ]\n",
    "        corrupt_labels = [pos for _, pos in tagged_words]\n",
    "\n",
    "        f1 = f1_score(true_labels, corrupt_labels, average='weighted')\n",
    "        f1_scores.append(f1)\n",
    "    \n",
    "    return error_counts, f1_scores\n",
    "\n",
    "\n",
    "random.seed(1234)\n",
    "errors, f1 = corruption_counts(model_set, 0.50, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation\n",
    "\n",
    "valid_set, model_set = train_test_split(tagged_sentences, train_size=0.1)\n",
    "\n",
    "# feature extraction    \n",
    "X_valid = []\n",
    "y_valid = []\n",
    "for sentence in valid_set:\n",
    "    X_sentence = []\n",
    "    y_sentence = []\n",
    "    for i in range(len(sentence)):\n",
    "        X_sentence.append(word_features(sentence, i))\n",
    "        y_sentence.append(sentence[i][1])\n",
    "    X_valid.append(X_sentence)\n",
    "    y_valid.append(y_sentence)    \n",
    "\n",
    "\n",
    "            \n",
    "# CV to find the best max iter\n",
    "best_param = train_and_tune_crf_with_cv(X_valid, y_valid, param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brill function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_and_evaluate_brill(data, num_repetitions, prob):\n",
    "\n",
    "    \"\"\"\n",
    "    Train and evaluate a Brill tagger.\n",
    "\n",
    "    Parameters:\n",
    "    - data: List of feature-label pairs.\n",
    "    - num_repetitions: number of times to repeat the experiment.\n",
    "    - prob: probability of randomly switching tags which will be passed to the corrupt_training_set function\n",
    "\n",
    "    Returns:\n",
    "    - f1_scores: list of f1 scores with length equal to num_repetitions.\n",
    "    - error_counts: A list of annotation errors induced with each run with length equal to num_repetitions.\n",
    "    \"\"\"\n",
    "    f1_scores = []\n",
    "    error_counts = []\n",
    "\n",
    "    for _ in range(num_repetitions):\n",
    "        # train test split\n",
    "        train_set, test_set = train_test_split(data, train_size=0.8)\n",
    "\n",
    "        # corruption\n",
    "        corrupt_train, error_count = corrupt_training_set(train_set, switch_prob=prob)\n",
    "        error_counts.append(error_count)\n",
    "\n",
    "        tag1 = DefaultTagger('NOUN')\n",
    "        unigram_tagger = UnigramTagger(corrupt_train, backoff=tag1)\n",
    "\n",
    "        templates = nltk.brill.nltkdemo18()\n",
    "        trainer = BrillTaggerTrainer(templates=templates, initial_tagger=unigram_tagger)\n",
    "\n",
    "        # Train the Brill Tagger using the templates\n",
    "        brill_tagger = trainer.train(corrupt_train, max_rules=200)\n",
    "\n",
    "        # get predictions\n",
    "        test_untagged_words = [tup[0] for sent in test_set for tup in sent]\n",
    "        tags = brill_tagger.tag(test_untagged_words)\n",
    "        brill_preds = [tag for  _,tag in tags]\n",
    "\n",
    "        test_true_tags = [tup[1] for sent in test_set for tup in sent]\n",
    "        f1 = f1_score(brill_preds, test_true_tags, average='weighted')\n",
    "        \n",
    "        \n",
    "\n",
    "        f1 = f1_score(test_true_tags, brill_preds, average='weighted')\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    return f1_scores, error_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unigram function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_unigram(data, num_repetitions, prob):\n",
    "\n",
    "    \"\"\"\n",
    "    Train and evaluate a Unigram tagger.\n",
    "\n",
    "    Parameters:\n",
    "    - data: List of feature-label pairs.\n",
    "    - num_repetitions: number of times to repeat the experiment.\n",
    "    - prob: probability of randomly switching tags which will be passed to the corrupt_training_set function\n",
    "\n",
    "    Returns:\n",
    "    - f1_scores: list of f1 scores with length equal to num_repetitions.\n",
    "    - error_counts: A list of annotation errors induced with each run with length equal to num_repetitions.\n",
    "    \"\"\"\n",
    "    \n",
    "    f1_scores = []\n",
    "    error_counts = []\n",
    "\n",
    "    for _ in range(num_repetitions):\n",
    "        # train test split\n",
    "        train_set, test_set = train_test_split(data, train_size=0.8)\n",
    "\n",
    "        # corruption\n",
    "        corrupt_train, error_count = corrupt_training_set(train_set, switch_prob=prob)\n",
    "        error_counts.append(error_count)\n",
    "\n",
    "        tag1 = DefaultTagger('NOUN')\n",
    "        unigram_tagger = UnigramTagger(corrupt_train, backoff=tag1)\n",
    "\n",
    "     \n",
    "        # get predictions\n",
    "        test_untagged_words = [tup[0] for sent in test_set for tup in sent]\n",
    "        unigram_tags = unigram_tagger.tag(test_untagged_words)\n",
    "        unigram_preds = [tag for  _,tag in unigram_tags]\n",
    "\n",
    "        test_true_tags = [tup[1] for sent in test_set for tup in sent]\n",
    "\n",
    "        f1 = f1_score(test_true_tags, unigram_preds, average='weighted')\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    return f1_scores, error_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HMM function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_hmm(data, num_repetitions, prob):\n",
    "\n",
    "    \"\"\"\n",
    "    Train and evaluate an HMM tagger.\n",
    "\n",
    "    Parameters:\n",
    "    - data: List of feature-label pairs.\n",
    "    - num_repetitions: number of times to repeat the experiment.\n",
    "    - prob: probability of randomly switching tags which will be passed to the corrupt_training_set function\n",
    "\n",
    "    Returns:\n",
    "    - f1_scores: list of f1 scores with length equal to num_repetitions.\n",
    "    - error_counts: A list of annotation errors induced with each run with length equal to num_repetitions.\n",
    "    \"\"\"\n",
    "        \n",
    "    f1_scores = []\n",
    "    error_counts = []\n",
    "\n",
    "    for _ in range(num_repetitions):\n",
    "        # train test split\n",
    "        train_set, test_set = train_test_split(data, train_size=0.8)\n",
    "\n",
    "        # corruption\n",
    "        corrupt_train, error_count = corrupt_training_set(train_set, switch_prob=prob)\n",
    "        error_counts.append(error_count)\n",
    "\n",
    "                 \n",
    "        tagger = nltk.HiddenMarkovModelTagger.train(corrupt_train)\n",
    "        \n",
    "        predicted_labels = []\n",
    "        correct_labels = []\n",
    "\n",
    "        for i, sent in enumerate(test_set):\n",
    "            predicted_labels += [tag for _, tag in tagger.tag([word for word, _ in sent])]\n",
    "            correct_labels += [tag for _, tag in sent]\n",
    "\n",
    "        f1 = f1_score(correct_labels, predicted_labels, average='weighted')\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    return f1_scores, error_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MEMM function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and evaluating function\n",
    "\n",
    "def train_and_evaluate_memm(data, num_repetitions, prob, max_iters):\n",
    "\n",
    "    \"\"\"\n",
    "    Train and evaluate an MEMM tagger.\n",
    "\n",
    "    Parameters:\n",
    "    - data: List of feature-label pairs.\n",
    "    - num_repetitions: number of times to repeat the experiment.\n",
    "    - prob: probability of randomly switching tags which will be passed to the corrupt_training_set function\n",
    "    - max_iters: maximum iterations for the MaxentClassifier \n",
    "\n",
    "    Returns:\n",
    "    - f1_scores: list of f1 scores with length equal to num_repetitions.\n",
    "    - error_counts: A list of annotation errors induced with each run with length equal to num_repetitions.\n",
    "    \"\"\"\n",
    "    # initialize list to keep track of model performance\n",
    "    f1_scores = []\n",
    "    error_counts = []\n",
    "\n",
    "\n",
    "    for _ in range(num_repetitions):\n",
    "\n",
    "        # train test split and formatting\n",
    "        train_set, test_set = train_test_split(data, train_size=0.8)\n",
    "        \n",
    "        # corrupt train set\n",
    "        corrupt_train, error_count = corrupt_training_set(train_set, switch_prob=prob)\n",
    "        error_counts.append(error_count)\n",
    "        \n",
    "        # Extract features\n",
    "        X_train = []\n",
    "        y_train = []\n",
    "        for sentence in corrupt_train:\n",
    "            X_sentence = []\n",
    "            y_sentence = []\n",
    "            for i in range(len(sentence)):\n",
    "                X_sentence.append(word_features(sentence, i))\n",
    "                y_sentence.append(sentence[i][1])\n",
    "            X_train.append(X_sentence)\n",
    "            y_train.append(y_sentence)\n",
    "\n",
    "        X_test = []\n",
    "        y_test = []\n",
    "        for sentence in test_set:\n",
    "            X_sentence = []\n",
    "            y_sentence = []\n",
    "            for i in range(len(sentence)):\n",
    "                X_sentence.append(word_features(sentence, i))\n",
    "                y_sentence.append(sentence[i][1])\n",
    "            X_test.append(X_sentence)\n",
    "            y_test.append(y_sentence)\n",
    "        \n",
    "             \n",
    "        # gather again\n",
    "        MEMM_train = []  # Collect feature-label pairs for MEMM\n",
    "        for sentence_features, sentence_labels in zip(X_train, y_train):\n",
    "            MEMM_train.extend(list(zip(sentence_features, sentence_labels)))\n",
    "\n",
    "        MEMM_test = []  # Collect feature-label pairs for MEMM\n",
    "        for sentence_features, sentence_labels in zip(X_test, y_test):\n",
    "            MEMM_test.extend(list(zip(sentence_features, sentence_labels)))\n",
    "\n",
    "\n",
    "        # training using the tuned value\n",
    "        maxent_classifier = MaxentClassifier.train(MEMM_train, algorithm='gis', max_iter=max_iters)\n",
    "\n",
    "        # predictions \n",
    "        memm_predictions = maxent_classifier.classify_many([features for features, _ in MEMM_test])\n",
    "\n",
    "        memm_true_labels = [pos for _, pos in MEMM_test]\n",
    "        \n",
    "        \n",
    "        f1 = f1_score(memm_true_labels, memm_predictions, average='weighted')\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "\n",
    "    return f1_scores, error_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CRF function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and evaluate\n",
    "def train_and_evaluate_crf(data, num_repetitions, prob, param_grid):\n",
    "\n",
    "    \"\"\"\n",
    "    Train and evaluate a CRF tagger.\n",
    "\n",
    "    Parameters:\n",
    "    - data: List of feature-label pairs.\n",
    "    - num_repetitions: number of times to repeat the experiment.\n",
    "    - prob: probability of randomly switching tags which will be passed to the corrupt_training_set function\n",
    "    - param_grid: dictionary of parameter specifications for the maximum number of iterations and regularization parameters:\n",
    "\n",
    "    Returns:\n",
    "    - f1_scores: list of f1 scores with length equal to num_repetitions.\n",
    "    - error_counts: A list of annotation errors induced with each run with length equal to num_repetitions.\n",
    "    \"\"\"\n",
    "        \n",
    "    # initialize list to keep track of model performance\n",
    "    f1_scores = []\n",
    "    error_counts = []\n",
    " \n",
    "    for _ in range(num_repetitions):\n",
    "\n",
    "        # train test split and formatting\n",
    "        train_set, test_set = train_test_split(data, train_size=0.8)\n",
    "        \n",
    "        # corrupt train set\n",
    "        corrupt_train, error_count = corrupt_training_set(train_set, switch_prob=prob)\n",
    "        error_counts.append(error_count)\n",
    "        \n",
    "        # Extract features\n",
    "        X_train = []\n",
    "        y_train = []\n",
    "        for sentence in corrupt_train:\n",
    "            X_sentence = []\n",
    "            y_sentence = []\n",
    "            for i in range(len(sentence)):\n",
    "                X_sentence.append(word_features(sentence, i))\n",
    "                y_sentence.append(sentence[i][1])\n",
    "            X_train.append(X_sentence)\n",
    "            y_train.append(y_sentence)\n",
    "\n",
    "        X_test = []\n",
    "        y_test = []\n",
    "        for sentence in test_set:\n",
    "            X_sentence = []\n",
    "            y_sentence = []\n",
    "            for i in range(len(sentence)):\n",
    "                X_sentence.append(word_features(sentence, i))\n",
    "                y_sentence.append(sentence[i][1])\n",
    "            X_test.append(X_sentence)\n",
    "            y_test.append(y_sentence)\n",
    "        \n",
    "                \n",
    "\n",
    "        # training using the tuned value\n",
    "        trainer = pycrfsuite.Trainer(verbose=False)\n",
    "\n",
    "        # Add training data\n",
    "        for x, y in zip(X_train, y_train):\n",
    "\t        trainer.append(x, y)\n",
    "\n",
    "        # Set trainer parameters\n",
    "        trainer.set_params(param_grid)\n",
    "\n",
    "        # Train the CRF model\n",
    "        trainer.train('pos.crfsuite')\n",
    "\n",
    "\n",
    "        # Testing\n",
    "        # Initialize the tagger\n",
    "        tagger = pycrfsuite.Tagger()\n",
    "        tagger.open('pos.crfsuite')\n",
    "\n",
    "\n",
    "        # predictions\n",
    "        CRF_predictions = [tagger.tag(instance) for instance in X_test]\n",
    "\n",
    "        CRF_flat_predictions = [tag for instance_tags in CRF_predictions for tag in instance_tags]\n",
    "        CRF_flat_ground_truth = [tag for instance_tags in y_test for tag in instance_tags]\n",
    "\n",
    "        # test score\n",
    "        f1 = f1_score(CRF_flat_ground_truth, CRF_flat_predictions, average='weighted')\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "\n",
    "    return f1_scores, error_counts\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collecting all results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random.seed(1234)\n",
    "\n",
    "\n",
    "# unigram_clean, unigram_errors_clean = train_and_evaluate_unigram(data=model_set, num_repetitions=5, prob=0)\n",
    "# unigram_5, unigram_errors_5 = train_and_evaluate_unigram(data=model_set, num_repetitions=5, prob=0.05)\n",
    "# unigram_20, unigram_errors_20 = train_and_evaluate_unigram(data=model_set, num_repetitions=5, prob=0.2)\n",
    "# unigram_35, unigram_errors_35 = train_and_evaluate_unigram(data=model_set, num_repetitions=5, prob=0.35)\n",
    "# unigram_50, unigram_errors_50 = train_and_evaluate_unigram(data=model_set, num_repetitions=5, prob=0.5)\n",
    "\n",
    "# brill_clean, brill_errors_clean = train_and_evaluate_brill(data=model_set, num_repetitions=5, prob=0)\n",
    "# brill_5, brill_errors_5 = train_and_evaluate_brill(data=model_set, num_repetitions=5, prob=0.05)\n",
    "# brill_20, brill_errors_20 = train_and_evaluate_brill(data=model_set, num_repetitions=5, prob=0.2)\n",
    "# brill_35, brill_errors_35 = train_and_evaluate_brill(data=model_set, num_repetitions=5, prob=0.35)\n",
    "# brill_50, brill_errors_50 = train_and_evaluate_brill(data=model_set, num_repetitions=5, prob=0.5)\n",
    "\n",
    "# hmm_clean, hmm_errors_clean = train_and_evaluate_hmm(data=model_set, num_repetitions=5, prob=0)\n",
    "# hmm_5, hmm_errors_5 = train_and_evaluate_hmm(data=model_set, num_repetitions=5, prob=0.05)\n",
    "# hmm_20, hmm_errors_20 = train_and_evaluate_hmm(data=model_set, num_repetitions=5, prob=0.2)\n",
    "# hmm_35, hmm_errors_35 = train_and_evaluate_hmm(data=model_set, num_repetitions=5, prob=0.35)\n",
    "# hmm_50, hmm_errors_50 = train_and_evaluate_hmm(data=model_set, num_repetitions=5, prob=0.5)\n",
    "\n",
    "# memm_clean, memm_errors_clean = train_and_evaluate_memm(data=model_set, num_repetitions=5, prob=0, max_iters=best_max_iter)\n",
    "# memm_5, memm_errors_5 = train_and_evaluate_memm(data=model_set, num_repetitions=5, prob=0.05, max_iters=best_max_iter)\n",
    "# memm_20, memm_errors_20 = train_and_evaluate_memm(data=model_set, num_repetitions=5, prob=0.2, max_iters=best_max_iter)\n",
    "# memm_35, memm_errors_35 = train_and_evaluate_memm(data=model_set, num_repetitions=5, prob=0.35, max_iters=best_max_iter)\n",
    "# memm_50, memm_errors_50 = train_and_evaluate_memm(data=model_set, num_repetitions=5, prob=0.5, max_iters=best_max_iter)\n",
    "\n",
    "# crf_clean, crf_errors_clean = train_and_evaluate_crf(data=model_set, num_repetitions=5, prob=0, param_grid=best_param)\n",
    "# crf_5, crf_errors_5 = train_and_evaluate_crf(data=model_set, num_repetitions=5, prob=0.05, param_grid=best_param)\n",
    "# crf_20, crf_errors_20 = train_and_evaluate_crf(data=model_set, num_repetitions=5, prob=0.2, param_grid=best_param)\n",
    "# crf_35, crf_errors_35 = train_and_evaluate_crf(data=model_set, num_repetitions=5, prob=0.35, param_grid=best_param)\n",
    "# crf_50, crf_errors_50 = train_and_evaluate_crf(data=model_set, num_repetitions=5, prob=0.5, param_grid=best_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# f1_scores = {\n",
    "#     \"hmm_clean\": hmm_clean,\n",
    "#     \"hmm_5\": hmm_5,\n",
    "#     \"hmm_20\": hmm_20,\n",
    "#     \"hmm_35\": hmm_35,\n",
    "#     \"hmm_50\": hmm_50,\n",
    "#     \"memm_clean\": memm_clean,\n",
    "#     \"memm_5\": memm_5,\n",
    "#     \"memm_20\": memm_20,\n",
    "#     \"memm_35\": memm_35,\n",
    "#     \"memm_50\": memm_50,\n",
    "#     \"crf_clean\": crf_clean,\n",
    "#     \"crf_5\": crf_5,\n",
    "#     \"crf_20\": crf_20,\n",
    "#     \"crf_35\": crf_35,\n",
    "#     \"crf_50\": crf_50,\n",
    "#     \"unigram_clean\": unigram_clean,\n",
    "#     \"unigram_5\": unigram_5,\n",
    "#     \"unigram_20\": unigram_20,\n",
    "#     \"unigram_35\": unigram_35,\n",
    "#     \"unigram_50\": unigram_50,\n",
    "#     \"brill_clean\": brill_clean,\n",
    "#     \"brill_5\": brill_5,\n",
    "#     \"brill_20\": brill_20,\n",
    "#     \"brill_35\": brill_35,\n",
    "#     \"brill_50\": brill_50\n",
    "# }\n",
    "\n",
    "# with open('insert file path here', \"w\") as file:\n",
    "#     for model_threshold, f1_score in f1_scores.items():\n",
    "#         file.write(f\"{model_threshold}: {f1_score}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('insert file path here', \"w\") as file:\n",
    "#     for param, paramval in best_param.items():\n",
    "#         file.write(f\"{param}: {paramval}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_it = {\n",
    "#     \"max_it\": best_max_iter\n",
    "# }\n",
    "\n",
    "# with open('insert file path here', \"w\") as file:\n",
    "#     for param, paramval in max_it.items():\n",
    "#         file.write(f\"{param}: {paramval}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
