{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#from data_loader import load_word_dataset\n",
    "import random\n",
    "import numpy as np\n",
    "import sys\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "import string\n",
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import pprint, time\n",
    "\n",
    "\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "from nltk.corpus import treebank\n",
    "from nltk.corpus import brown\n",
    "\n",
    "from nltk.classify import MaxentClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "import pycrfsuite\n",
    "from nltk.tag import hmm\n",
    "# from nltk.classify import megam\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from nltk.tag import BrillTaggerTrainer\n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.tag import DefaultTagger\n",
    "from nltk.metrics import ConfusionMatrix\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading treebank: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    }
   ],
   "source": [
    "#download the treebank corpus from nltk\n",
    "\n",
    "nltk.download('treebank')\n",
    "  \n",
    "# reading the Treebank tagged sentences\n",
    "tagged_sentences = list(nltk.corpus.treebank.tagged_sents(tagset='universal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of nouns: 0.2867\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "# Flatten the list of tagged words\n",
    "tagged_words = [word for sentence in tagged_sentences for word in sentence]\n",
    "\n",
    "# Count the occurrences of each POS tag\n",
    "pos_counts = Counter(tag for word,tag in tagged_words)\n",
    "\n",
    "# Get the total number of words\n",
    "total_words = len(tagged_words)\n",
    "\n",
    "# Calculate the proportion of nouns\n",
    "noun_proportion = pos_counts['NOUN'] / total_words\n",
    "\n",
    "print(f\"Proportion of nouns: {noun_proportion:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_and_evaluate_brill(data, num_repetitions, train_prop):\n",
    "\n",
    "    \n",
    "    f1_scores = []\n",
    "\n",
    "    for _ in range(num_repetitions):\n",
    "        train_set, test_set = train_test_split(data, train_size=train_prop)\n",
    "\n",
    "        tag1 = DefaultTagger('X')\n",
    "        unigram_tagger = UnigramTagger(train_set, backoff=tag1)\n",
    "\n",
    "        templates = nltk.brill.nltkdemo18()\n",
    "        trainer = BrillTaggerTrainer(templates=templates, initial_tagger=unigram_tagger)\n",
    "\n",
    "        # Train the Brill Tagger using the templates\n",
    "        brill_tagger = trainer.train(train_set, max_rules=200)\n",
    "\n",
    "        # get predictions\n",
    "        test_untagged_words = [tup[0] for sent in test_set for tup in sent]\n",
    "        tags = brill_tagger.tag(test_untagged_words)\n",
    "        brill_preds = [tag for  _,tag in tags]\n",
    "\n",
    "        test_true_tags = [tup[1] for sent in test_set for tup in sent]\n",
    "        #f1 = f1_score(brill_preds, test_true_tags, average='weighted')\n",
    "        \n",
    "        \n",
    "\n",
    "        f1 = f1_score(test_true_tags, brill_preds, average='weighted')\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    return f1_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_unigram(data, num_repetitions, train_prop):\n",
    "\n",
    "    \n",
    "    f1_scores = []\n",
    "\n",
    "    for _ in range(num_repetitions):\n",
    "        train_set, test_set = train_test_split(data, train_size=train_prop)\n",
    "\n",
    "        tag1 = DefaultTagger('X')\n",
    "        unigram_tagger = UnigramTagger(train_set, backoff=tag1)\n",
    "\n",
    "     \n",
    "        # get predictions\n",
    "        test_untagged_words = [tup[0] for sent in test_set for tup in sent]\n",
    "        unigram_tags = unigram_tagger.tag(test_untagged_words)\n",
    "        unigram_preds = [tag for  _,tag in unigram_tags]\n",
    "\n",
    "        test_true_tags = [tup[1] for sent in test_set for tup in sent]\n",
    "\n",
    "        f1 = f1_score(test_true_tags, unigram_preds, average='weighted')\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    return f1_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_hmm(data, num_repetitions, train_prop):\n",
    "\n",
    "    \n",
    "    f1_scores = []\n",
    "\n",
    "    for _ in range(num_repetitions):\n",
    "        train_set, test_set = train_test_split(data, train_size=train_prop)\n",
    "        tagger = nltk.HiddenMarkovModelTagger.train(train_set)\n",
    "        \n",
    "        predicted_labels = []\n",
    "        correct_labels = []\n",
    "\n",
    "        for i, sent in enumerate(test_set):\n",
    "            predicted_labels += [tag for _, tag in tagger.tag([word for word, _ in sent])]\n",
    "            correct_labels += [tag for _, tag in sent]\n",
    "\n",
    "        f1 = f1_score(correct_labels, predicted_labels, average='weighted')\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    return f1_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gathering results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random.seed(1234)\n",
    "\n",
    "\n",
    "# unigram_80 = train_and_evaluate_unigram(data=tagged_sentences, num_repetitions=10, train_prop=0.8)\n",
    "# brill_80 = train_and_evaluate_brill(data=tagged_sentences, num_repetitions=10, train_prop=0.8)\n",
    "# hmm_80 = train_and_evaluate_hmm(data=tagged_sentences, num_repetitions=10, train_prop=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1_scores = {\n",
    "#     \"hmm_80\": hmm_80,\n",
    "#     \"unigram_80\": unigram_80,\n",
    "#     \"brill_80\": brill_80\n",
    "# }\n",
    "\n",
    "\n",
    "\n",
    "# with open('insert file path here', \"w\") as file:\n",
    "#     for model_threshold, f1_score in f1_scores.items():\n",
    "#         file.write(f\"{model_threshold}: {f1_score}/n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
