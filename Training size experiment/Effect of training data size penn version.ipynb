{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pprint, time\n",
    "\n",
    "import nltk\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "from nltk.corpus import treebank\n",
    "\n",
    "\n",
    "from nltk.classify import MaxentClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "import pycrfsuite\n",
    "from nltk.tag import hmm\n",
    "from nltk.classify import megam\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from nltk.tag import BrillTaggerTrainer\n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.tag import DefaultTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\21947074\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\21947074\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3914\n"
     ]
    }
   ],
   "source": [
    "#download the treebank corpus from nltk\n",
    "nltk.download('treebank')\n",
    " \n",
    "#download the universal tagset from nltk\n",
    "nltk.download('universal_tagset')\n",
    " \n",
    "#reading the Treebank tagged sentences\n",
    "tagged_sentences = list(nltk.corpus.treebank.tagged_sents())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brill function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_and_evaluate_brill(data, num_repetitions, train_prop):\n",
    "    \n",
    "    \"\"\"\n",
    "    Train and evaluate a Brill tagger.\n",
    "\n",
    "    Parameters:\n",
    "    - data: List of feature-label pairs.\n",
    "    - num_repetitions: number of times to repeat the experiment.\n",
    "    - train_prop: proportion of training data.\n",
    "\n",
    "    Returns:\n",
    "    - f1_scores: list of f1 scores with length equal to num_repetitions.\n",
    "    \"\"\"\n",
    "    \n",
    "    f1_scores = []\n",
    "\n",
    "    for _ in range(num_repetitions):\n",
    "        train_set, test_set = train_test_split(data, train_size=train_prop)\n",
    "\n",
    "        tag1 = DefaultTagger('NN')\n",
    "        unigram_tagger = UnigramTagger(train_set, backoff=tag1)\n",
    "\n",
    "        templates = nltk.brill.nltkdemo18()\n",
    "        trainer = BrillTaggerTrainer(templates=templates, initial_tagger=unigram_tagger)\n",
    "\n",
    "        # Train the Brill Tagger using the templates\n",
    "        brill_tagger = trainer.train(train_set, max_rules=200)\n",
    "\n",
    "        # get predictions\n",
    "        test_untagged_words = [tup[0] for sent in test_set for tup in sent]\n",
    "        tags = brill_tagger.tag(test_untagged_words)\n",
    "        brill_preds = [tag for  _,tag in tags]\n",
    "\n",
    "        test_true_tags = [tup[1] for sent in test_set for tup in sent]\n",
    "           \n",
    "        \n",
    "        f1 = f1_score(test_true_tags, brill_preds, average='weighted')\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    return f1_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unigram function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_unigram(data, num_repetitions, train_prop):\n",
    "\n",
    "    \"\"\"\n",
    "    Train and evaluate a Unigram tagger.\n",
    "\n",
    "    Parameters:\n",
    "    - data: List of feature-label pairs.\n",
    "    - num_repetitions: number of times to repeat the experiment.\n",
    "    - train_prop: proportion of training data.\n",
    "\n",
    "    Returns:\n",
    "    - f1_scores: list of f1 scores with length equal to num_repetitions.\n",
    "    \"\"\"\n",
    "        \n",
    "    f1_scores = []\n",
    "\n",
    "    for _ in range(num_repetitions):\n",
    "        train_set, test_set = train_test_split(data, train_size=train_prop)\n",
    "\n",
    "        tag1 = DefaultTagger('NN')\n",
    "        unigram_tagger = UnigramTagger(train_set, backoff=tag1)\n",
    "\n",
    "     \n",
    "        # get predictions\n",
    "        test_untagged_words = [tup[0] for sent in test_set for tup in sent]\n",
    "        unigram_tags = unigram_tagger.tag(test_untagged_words)\n",
    "        unigram_preds = [tag for  _,tag in unigram_tags]\n",
    "\n",
    "        test_true_tags = [tup[1] for sent in test_set for tup in sent]\n",
    "\n",
    "        f1 = f1_score(test_true_tags, unigram_preds, average='weighted')\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    return f1_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature extraction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_features(sentence, i):\n",
    "\n",
    "    \"\"\"\n",
    "    Extract features for a given index in a sentence.\n",
    "\n",
    "    Parameters:\n",
    "    - sentence: List of feature-label pairs.\n",
    "    - i: index\n",
    "\n",
    "    Returns:\n",
    "    - features: a dictionary of features on a given index.\n",
    "    \"\"\"\n",
    "        \n",
    "    word = sentence[i][0]\n",
    "    tag = sentence[i][1]\n",
    "\n",
    "    features = {\n",
    "        'word': word,\n",
    "        'is_first': i == 0,  # if the word is the first word\n",
    "        'is_last': i == len(sentence) - 1,  # if the word is the last word\n",
    "        'is_capitalized': word[0].upper() == word[0],\n",
    "        'is_all_caps': word.upper() == word,  # word is in uppercase\n",
    "        'is_all_lower': word.lower() == word,  # word is in lowercase\n",
    "        # prefix of the word\n",
    "        'prefix-1': word[0],\n",
    "        'prefix-2': word[:2],\n",
    "        'prefix-3': word[:3],\n",
    "        # suffix of the word\n",
    "        'suffix-1': word[-1],\n",
    "        'suffix-2': word[-2:],\n",
    "        'suffix-3': word[-3:],\n",
    "        # extracting previous word\n",
    "        'prev_word': '' if i == 0 else sentence[i - 1][0],\n",
    "        # extracting next word\n",
    "        'next_word': '' if i == len(sentence) - 1 else sentence[i + 1][0],\n",
    "        'has_hyphen': '-' in word,  # if word has a hyphen\n",
    "        'is_numeric': word.isdigit(),  # if word is numeric\n",
    "        'capitals_inside': word[1:].lower() != word[1:]\n",
    "    }\n",
    "\n",
    "    # Add previous tag and its previous tag\n",
    "    prev_tag = '' if i == 0 else sentence[i - 1][1]\n",
    "    prev_prev_tag = '' if i < 2 else sentence[i - 2][1]\n",
    "    features['prev_prev_tag'] = f'{prev_prev_tag}_{prev_tag}'\n",
    "\n",
    "    # Add word after the next word\n",
    "    features['next_next_word'] = '' if i > len(sentence) - 3 else sentence[i + 2][0]\n",
    "\n",
    "    # Add word before the previous word\n",
    "    features['prev_prev_word'] = '' if i < 2 else sentence[i - 2][0]\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract CRF features\n",
    "X = []\n",
    "y = []\n",
    "for sentence in tagged_sentences:\n",
    "\tX_sentence = []\n",
    "\ty_sentence = []\n",
    "\tfor i in range(len(sentence)):\n",
    "\t\tX_sentence.append(word_features(sentence, i))\n",
    "\t\ty_sentence.append(sentence[i][1])\n",
    "\tX.append(X_sentence)\n",
    "\ty.append(y_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def train_and_tune_memm(MEMM_train, max_iter_values, num_folds=5):\n",
    "    \n",
    "    \"\"\"\n",
    "    Train and tune a Maximum Entropy Markov Model (MEMM) using cross-validation.\n",
    "\n",
    "    Parameters:\n",
    "    - MEMM_train: List of feature-label pairs for training.\n",
    "    - max_iter_values: List of max_iter values to tune.\n",
    "    - num_folds: Number of folds for cross-validation.\n",
    "\n",
    "    Returns:\n",
    "    - best_max_iter: The best max_iter value found.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize variables to keep track of the best max_iter and its associated F1 score\n",
    "    best_max_iter = None\n",
    "    best_f1 = 0.0\n",
    "\n",
    "    # Define the number of folds for cross-validation\n",
    "    kf = KFold(n_splits=num_folds)\n",
    "\n",
    "    for max_iter in max_iter_values:\n",
    "        f1_scores = []\n",
    "\n",
    "        for train_idx, valid_idx in kf.split(MEMM_train):\n",
    "            train_set = [MEMM_train[i] for i in train_idx]\n",
    "            valid_set = [MEMM_train[i] for i in valid_idx]\n",
    "\n",
    "            maxent_classifier = MaxentClassifier.train(train_set, algorithm='gis', max_iter=max_iter)\n",
    "\n",
    "            valid_features = [features for features, _ in valid_set]\n",
    "            valid_labels = [pos for _, pos in valid_set]\n",
    "\n",
    "            predictions = [maxent_classifier.classify(features) for features in valid_features]\n",
    "\n",
    "            f1 = f1_score(valid_labels, predictions, average='weighted')\n",
    "            f1_scores.append(f1)\n",
    "\n",
    "        # Calculate the average F1 score across folds\n",
    "        avg_f1 = np.mean(f1_scores)\n",
    "\n",
    "        # Check if this max_iter gives a better F1 score than the current best\n",
    "        if avg_f1 > best_f1:\n",
    "            best_max_iter = max_iter\n",
    "            best_f1 = avg_f1\n",
    "\n",
    "    return best_max_iter\n",
    "\n",
    "# Example usage:\n",
    "# best_max_iter, best_f1 = train_and_tune_memm(MEMM_train40, max_iter_values=[5, 10, 15, 20])\n",
    "# print(f\"Best max_iter: {best_max_iter}\")\n",
    "# print(f\"Best F1 Score: {best_f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_and_tune_crf_with_cv(X, y, param_grid, n_folds=5):\n",
    "\n",
    "    \"\"\"\n",
    "    Train and tune a Conditional Random Fields (CRF) Model using cross-validation.\n",
    "\n",
    "    Parameters:\n",
    "    - X: list of extracted features\n",
    "    - y: list of corresponding tags\n",
    "    - param_grid: search grid dictionary.\n",
    "    - n_folds: Number of folds for cross-validation.\n",
    "\n",
    "    Returns:\n",
    "    - best_params: A dictionary of the best parameter values found.\n",
    "    \"\"\"\n",
    "    best_f1 = 0.0\n",
    "    best_params = {}\n",
    "\n",
    "    for max_iter in param_grid['max_iterations']:\n",
    "        for c1 in param_grid['c1']:\n",
    "            for c2 in param_grid['c2']:\n",
    "                f1_scores = []\n",
    "\n",
    "                for fold in range(n_folds):\n",
    "                    # Split data into training and validation sets\n",
    "                    train_indices = [i for i in range(len(X)) if i % n_folds != fold]\n",
    "                    valid_indices = [i for i in range(len(X)) if i % n_folds == fold]\n",
    "\n",
    "                    X_train_fold = [X[i] for i in train_indices]\n",
    "                    y_train_fold = [y[i] for i in train_indices]\n",
    "                    X_valid_fold = [X[i] for i in valid_indices]\n",
    "                    y_valid_fold = [y[i] for i in valid_indices]\n",
    "\n",
    "                    # Train the CRF model\n",
    "                    trainer = pycrfsuite.Trainer(verbose=False)\n",
    "                    for x_train, y_train in zip(X_train_fold, y_train_fold):\n",
    "                        trainer.append(x_train, y_train)\n",
    "                    trainer.set_params({\n",
    "                        'max_iterations': max_iter,\n",
    "                        'c1': c1,\n",
    "                        'c2': c2,\n",
    "                        'feature.possible_transitions': True\n",
    "                    })\n",
    "                    trainer.train('temp_model.crfsuite')\n",
    "\n",
    "                    # Test the CRF model\n",
    "                    tagger = pycrfsuite.Tagger()\n",
    "                    tagger.open('temp_model.crfsuite')\n",
    "\n",
    "                    CRF_predictions = [tagger.tag(instance) for instance in X_valid_fold]\n",
    "\n",
    "                    CRF_flat_predictions = [tag for instance_tags in CRF_predictions for tag in instance_tags]\n",
    "                    CRF_flat_ground_truth = [tag for instance_tags in y_valid_fold for tag in instance_tags]\n",
    "\n",
    "                    f1 = f1_score(CRF_flat_ground_truth, CRF_flat_predictions, average='weighted')\n",
    "                    f1_scores.append(f1)\n",
    "\n",
    "                mean_f1 = np.mean(f1_scores)\n",
    "                if mean_f1 > best_f1:\n",
    "                    best_f1 = mean_f1\n",
    "                    best_params = {\n",
    "                        'max_iterations': max_iter,\n",
    "                        'c1': c1,\n",
    "                        'c2': c2,\n",
    "                        'feature.possible_transitions': True\n",
    "                    }\n",
    "\n",
    "    #print(\"Best Parameters: \", best_params)\n",
    "    #print(\"Best F1 Score: \", best_f1)\n",
    "\n",
    "    return best_params\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation\n",
    "\n",
    "random.seed(1234)\n",
    "valid_set, model_set = train_test_split(tagged_sentences, train_size=0.1)\n",
    "\n",
    "# feature extraction    \n",
    "X_valid = []\n",
    "y_valid = []\n",
    "for sentence in valid_set:\n",
    "    X_sentence = []\n",
    "    y_sentence = []\n",
    "    for i in range(len(sentence)):\n",
    "        X_sentence.append(word_features(sentence, i))\n",
    "        y_sentence.append(sentence[i][1])\n",
    "    X_valid.append(X_sentence)\n",
    "    y_valid.append(y_sentence)    \n",
    "\n",
    "MEMM_valid = []  # Collect feature-label pairs for MEMM\n",
    "for sentence_features, sentence_labels in zip(X_valid, y_valid):\n",
    "    MEMM_valid.extend(list(zip(sentence_features, sentence_labels)))  \n",
    "\n",
    "best_max_iter = train_and_tune_memm(MEMM_valid, max_iter_values=[10,20,30])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the parameter grid for tuning crf\n",
    "param_grid = {\n",
    "    'max_iterations': [20, 50, 100],\n",
    "    'c1': [0.01, 0.1, 1.0, 10],\n",
    "    'c2': [1e-4, 1e-3, 1e-2, 1e-1]\n",
    "}\n",
    "\n",
    "# feature extraction    \n",
    "X_valid = []\n",
    "y_valid = []\n",
    "for sentence in valid_set:\n",
    "    X_sentence = []\n",
    "    y_sentence = []\n",
    "    for i in range(len(sentence)):\n",
    "        X_sentence.append(word_features(sentence, i))\n",
    "        y_sentence.append(sentence[i][1])\n",
    "    X_valid.append(X_sentence)\n",
    "    y_valid.append(y_sentence)    \n",
    "\n",
    "\n",
    "# validation\n",
    "best_param = train_and_tune_crf_with_cv(X_valid, y_valid, param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HMM function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_hmm(data, num_repetitions, train_prop):\n",
    "\n",
    "    \"\"\"\n",
    "    Train and evaluate an HMM tagger.\n",
    "\n",
    "    Parameters:\n",
    "    - data: List of feature-label pairs.\n",
    "    - num_repetitions: number of times to repeat the experiment.\n",
    "    - train_prop: proportion of training data.\n",
    "\n",
    "    Returns:\n",
    "    - f1_scores: list of f1 scores with length equal to num_repetitions.\n",
    "    \"\"\"\n",
    "        \n",
    "    f1_scores = []\n",
    "\n",
    "    for _ in range(num_repetitions):\n",
    "        train_set, test_set = train_test_split(data, train_size=train_prop)\n",
    "        tagger = nltk.HiddenMarkovModelTagger.train(train_set)\n",
    "        \n",
    "        predicted_labels = []\n",
    "        correct_labels = []\n",
    "\n",
    "        for i, sent in enumerate(test_set):\n",
    "            predicted_labels += [tag for _, tag in tagger.tag([word for word, _ in sent])]\n",
    "            correct_labels += [tag for _, tag in sent]\n",
    "\n",
    "        f1 = f1_score(correct_labels, predicted_labels, average='weighted')\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    return f1_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MEMM function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_memm(data, num_repetitions, train_prop, max_iters):\n",
    "\n",
    "    \"\"\"\n",
    "    Train and evaluate an MEMM tagger.\n",
    "\n",
    "    Parameters:\n",
    "    - data: List of feature-label pairs.\n",
    "    - num_repetitions: number of times to repeat the experiment.\n",
    "    - train_prop: proportion of training data.\n",
    "    - max_iters: maximum iterations for the MaxentClassifier \n",
    "\n",
    "    Returns:\n",
    "    - f1_scores: list of f1 scores with length equal to num_repetitions.\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize list to keep track of model performance\n",
    "    f1_scores = []\n",
    "    \n",
    "\n",
    "    for _ in range(num_repetitions):\n",
    "\n",
    "        # train test split and formatting\n",
    "        train_set, test_set = train_test_split(data, train_size=train_prop)\n",
    "\n",
    "        # feature extraction    \n",
    "        X_train = []\n",
    "        y_train = []\n",
    "        for sentence in train_set:\n",
    "            X_sentence = []\n",
    "            y_sentence = []\n",
    "            for i in range(len(sentence)):\n",
    "                X_sentence.append(word_features(sentence, i))\n",
    "                y_sentence.append(sentence[i][1])\n",
    "            X_train.append(X_sentence)\n",
    "            y_train.append(y_sentence) \n",
    "\n",
    "\n",
    "        X_test = []\n",
    "        y_test = []\n",
    "        for sentence in test_set:\n",
    "            X_sentence = []\n",
    "            y_sentence = []\n",
    "            for i in range(len(sentence)):\n",
    "                X_sentence.append(word_features(sentence, i))\n",
    "                y_sentence.append(sentence[i][1])\n",
    "            X_test.append(X_sentence)\n",
    "            y_test.append(y_sentence) \n",
    "        \n",
    "        \n",
    "        MEMM_train = []  # Collect feature-label pairs for MEMM\n",
    "        for sentence_features, sentence_labels in zip(X_train, y_train):\n",
    "            MEMM_train.extend(list(zip(sentence_features, sentence_labels)))\n",
    "\n",
    "        MEMM_test = []  # Collect feature-label pairs for MEMM\n",
    "        for sentence_features, sentence_labels in zip(X_test, y_test):\n",
    "            MEMM_test.extend(list(zip(sentence_features, sentence_labels)))\n",
    "\n",
    "        \n",
    "        # training using the tuned value\n",
    "        maxent_classifier = MaxentClassifier.train(MEMM_train, algorithm='gis', max_iter=max_iters)\n",
    "\n",
    "        # predictions \n",
    "        memm_predictions = maxent_classifier.classify_many([features for features, _ in MEMM_test])\n",
    "\n",
    "        memm_true_labels = [pos for _, pos in MEMM_test]\n",
    "        \n",
    "        \n",
    "        f1 = f1_score(memm_true_labels, memm_predictions, average='weighted')\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "\n",
    "    return f1_scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CRF function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_and_evaluate_crf(data, num_repetitions, train_prop, param_grid):\n",
    "\n",
    "    \"\"\"\n",
    "    Train and evaluate a CRF tagger.\n",
    "\n",
    "    Parameters:\n",
    "    - data: List of feature-label pairs.\n",
    "    - num_repetitions: number of times to repeat the experiment.\n",
    "    - train_prop: proportion of training data.\n",
    "    - param_grid: dictionary of parameter specifications for the maximum number of iterations and regularization parameters:\n",
    "\n",
    "    Returns:\n",
    "    - f1_scores: list of f1 scores with length equal to num_repetitions.\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize list to keep track of model performance\n",
    "    f1_scores = []\n",
    "\n",
    "\n",
    "\n",
    "    for _ in range(num_repetitions):\n",
    "\n",
    "         # train test split and formatting\n",
    "        train_set, test_set = train_test_split(data, train_size=train_prop)\n",
    "\n",
    "        # feature extraction    \n",
    "        X_train = []\n",
    "        y_train = []\n",
    "        for sentence in train_set:\n",
    "            X_sentence = []\n",
    "            y_sentence = []\n",
    "            for i in range(len(sentence)):\n",
    "                X_sentence.append(word_features(sentence, i))\n",
    "                y_sentence.append(sentence[i][1])\n",
    "            X_train.append(X_sentence)\n",
    "            y_train.append(y_sentence) \n",
    "\n",
    "\n",
    "        X_test = []\n",
    "        y_test = []\n",
    "        for sentence in test_set:\n",
    "            X_sentence = []\n",
    "            y_sentence = []\n",
    "            for i in range(len(sentence)):\n",
    "                X_sentence.append(word_features(sentence, i))\n",
    "                y_sentence.append(sentence[i][1])\n",
    "            X_test.append(X_sentence)\n",
    "            y_test.append(y_sentence)    \n",
    "   \n",
    "\n",
    "        \n",
    "\n",
    "        # training using the tuned value\n",
    "        trainer = pycrfsuite.Trainer(verbose=False)\n",
    "\n",
    "        # Add training data\n",
    "        for x, y in zip(X_train, y_train):\n",
    "\t        trainer.append(x, y)\n",
    "\n",
    "        # Set trainer parameters\n",
    "        trainer.set_params(param_grid)\n",
    "\n",
    "        # Train the CRF model\n",
    "        trainer.train('pos.crfsuite')\n",
    "\n",
    "\n",
    "        # Testing\n",
    "        # Initialize the tagger\n",
    "        tagger = pycrfsuite.Tagger()\n",
    "        tagger.open('pos.crfsuite')\n",
    "\n",
    "\n",
    "        # predictions\n",
    "        CRF_predictions = [tagger.tag(instance) for instance in X_test]\n",
    "\n",
    "        CRF_flat_predictions = [tag for instance_tags in CRF_predictions for tag in instance_tags]\n",
    "        CRF_flat_ground_truth = [tag for instance_tags in y_test for tag in instance_tags]\n",
    "\n",
    "        # test score\n",
    "        f1 = f1_score(CRF_flat_ground_truth, CRF_flat_predictions, average='weighted')\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "\n",
    "    return f1_scores\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collecting all results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run. Takes very long\n",
    "\n",
    "# random.seed(1234)\n",
    "\n",
    "\n",
    "# unigram_40 = train_and_evaluate_unigram(data=model_set, num_repetitions=5, train_prop=0.4)\n",
    "# unigram_60 = train_and_evaluate_unigram(data=model_set, num_repetitions=5, train_prop=0.6)\n",
    "# unigram_80 = train_and_evaluate_unigram(data=model_set, num_repetitions=5, train_prop=0.8)\n",
    "\n",
    "# brill_40 = train_and_evaluate_brill(data=model_set, num_repetitions=5, train_prop=0.4)\n",
    "# brill_60 = train_and_evaluate_brill(data=model_set, num_repetitions=5, train_prop=0.6)\n",
    "# brill_80 = train_and_evaluate_brill(data=model_set, num_repetitions=5, train_prop=0.8)\n",
    "\n",
    "# hmm_40 = train_and_evaluate_hmm(data=model_set, num_repetitions=5, train_prop=0.4)\n",
    "# hmm_60 = train_and_evaluate_hmm(data=model_set, num_repetitions=5, train_prop=0.6)\n",
    "# hmm_80 = train_and_evaluate_hmm(data=model_set, num_repetitions=5, train_prop=0.8)\n",
    "\n",
    "# memm_40 = train_and_evaluate_memm(data=model_set, num_repetitions=5, train_prop=0.4, max_iters=best_max_iter)\n",
    "# memm_60 = train_and_evaluate_memm(data=model_set, num_repetitions=5, train_prop=0.6, max_iters=best_max_iter)\n",
    "# memm_80 = train_and_evaluate_memm(data=model_set, num_repetitions=5, train_prop=0.8, max_iters=best_max_iter)\n",
    "\n",
    "# crf_40 = train_and_evaluate_crf(data=model_set, num_repetitions=5, train_prop=0.4, param_grid=best_param)\n",
    "# crf_60 = train_and_evaluate_crf(data=model_set, num_repetitions=5, train_prop=0.6, param_grid=best_param)\n",
    "# crf_80 = train_and_evaluate_crf(data=model_set, num_repetitions=5, train_prop=0.8, param_grid=best_param)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# f1_scores = {\n",
    "#     \"hmm_40\": hmm_40,\n",
    "#     \"hmm_60\": hmm_60,\n",
    "#     \"hmm_80\": hmm_80,\n",
    "#     \"memm_40\": memm_40,\n",
    "#     \"memm_60\": memm_60,\n",
    "#     \"memm_80\": memm_80,\n",
    "#     \"crf_40\": crf_40,\n",
    "#     \"crf_60\": crf_60,\n",
    "#     \"crf_80\": crf_80,\n",
    "#     \"unigram_40\": unigram_40,\n",
    "#     \"unigram_60\": unigram_60,\n",
    "#     \"unigram_80\": unigram_80,\n",
    "#     \"brill_40\": brill_40,\n",
    "#     \"brill_60\": brill_60,\n",
    "#     \"brill_80\": brill_80\n",
    "# }\n",
    "\n",
    "# with open('insert file path here', \"w\") as file:\n",
    "#     for model_threshold, f1_score in f1_scores.items():\n",
    "#         file.write(f\"{model_threshold}: {f1_score}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('insert file path here', \"w\") as file:\n",
    "#     for param, paramval in best_param.items():\n",
    "#         file.write(f\"{param}: {paramval}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_it = {\n",
    "#     \"max_it\": best_max_iter\n",
    "# }\n",
    "\n",
    "# with open('insert file path here', \"w\") as file:\n",
    "#     for param, paramval in max_it.items():\n",
    "#         file.write(f\"{param}: {paramval}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
