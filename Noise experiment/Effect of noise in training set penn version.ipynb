{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#from data_loader import load_word_dataset\n",
    "import random\n",
    "import numpy as np\n",
    "import sys\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "import string\n",
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import pprint, time\n",
    "\n",
    "\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "from nltk.corpus import treebank\n",
    "from nltk.corpus import brown\n",
    "\n",
    "from nltk.classify import MaxentClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "import pycrfsuite\n",
    "from nltk.tag import hmm\n",
    "# from nltk.classify import megam\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from nltk.tag import BrillTaggerTrainer\n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.tag import DefaultTagger\n",
    "from nltk.metrics import ConfusionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\kstap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#download the treebank corpus from nltk\n",
    "\n",
    "nltk.download('treebank')\n",
    "  \n",
    "# reading the Treebank tagged sentences\n",
    "tagged_sentences = list(nltk.corpus.treebank.tagged_sents())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to create noisy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splits: 0\n",
      "Joins: 0\n",
      "Original Sentence:\n",
      "[('But', 'ADP'), ('in', 'ADP'), ('the', 'DET'), ('three', 'NUM'), ('leading', 'VERB'), ('political', 'ADJ'), ('contests', 'NOUN'), ('of', 'ADP'), ('1989', 'NUM'), (',', '.'), ('the', 'DET'), ('negative', 'ADJ'), ('ads', 'NOUN'), ('have', 'VERB'), ('reached', 'VERB'), ('new', 'ADJ'), ('levels', 'NOUN'), ('of', 'ADP'), ('hostility', 'NOUN'), (',', '.'), ('*-1', 'X'), ('raising', 'VERB'), ('fears', 'NOUN'), ('that', 'ADP'), ('this', 'DET'), ('kind', 'NOUN'), ('of', 'ADP'), ('mudslinging', 'NOUN'), (',', '.'), ('empty', 'ADJ'), ('of', 'ADP'), ('significant', 'ADJ'), ('issues', 'NOUN'), (',', '.'), ('is', 'VERB'), ('ushering', 'VERB'), ('in', 'ADP'), ('a', 'DET'), ('new', 'ADJ'), ('era', 'NOUN'), ('of', 'ADP'), ('campaigns', 'NOUN'), ('without', 'ADP'), ('content', 'NOUN'), ('.', '.')]\n",
      "\n",
      "Modified Sentence:\n",
      "[('But', 'ADP'), ('in', 'ADP'), ('the', 'DET'), ('three', 'NUM'), ('leading', 'VERB'), ('political', 'ADJ'), ('contests', 'NOUN'), ('of', 'ADP'), ('1989', 'NUM'), (',', '.'), ('the', 'DET'), ('negative', 'ADJ'), ('ads', 'NOUN'), ('have', 'VERB'), ('reached', 'VERB'), ('new', 'ADJ'), ('levels', 'NOUN'), ('of', 'ADP'), ('hostility', 'NOUN'), (',', '.'), ('*-1', 'X'), ('raising', 'VERB'), ('fears', 'NOUN'), ('that', 'ADP'), ('this', 'DET'), ('kind', 'NOUN'), ('of', 'ADP'), ('mudslinging', 'NOUN'), (',', '.'), ('empty', 'ADJ'), ('of', 'ADP'), ('significant', 'ADJ'), ('issues', 'NOUN'), (',', '.'), ('is', 'VERB'), ('ushering', 'VERB'), ('in', 'ADP'), ('a', 'DET'), ('new', 'ADJ'), ('era', 'NOUN'), ('of', 'ADP'), ('campaigns', 'NOUN'), ('without', 'ADP'), ('content', 'NOUN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def modify_sentence_with_probability_and_count(corpus, join_probability, split_probability):\n",
    "    \"\"\"\n",
    "    Modify sentences in a corpus with specified join and split probabilities\n",
    "    and count the number of splits and joins.\n",
    "\n",
    "    Args:\n",
    "    - corpus: NLTK corpus object containing tagged sentences.\n",
    "    - join_probability: Probability of joining two adjacent words (0.0 to 1.0).\n",
    "    - split_probability: Probability of splitting a single word (0.0 to 1.0).\n",
    "\n",
    "    Returns:\n",
    "    - A modified corpus with words joined and split based on the specified probabilities.\n",
    "    - Counts of splits and joins as a tuple (split_count, join_count).\n",
    "    \"\"\"\n",
    "    modified_corpus = []\n",
    "    split_count = 0\n",
    "    join_count = 0\n",
    "\n",
    "    for tagged_sentence in corpus:\n",
    "        modified_sentence = []\n",
    "        i = 0\n",
    "\n",
    "        while i < len(tagged_sentence):\n",
    "            # Randomly decide whether to join the current word with the next word\n",
    "            if random.random() < join_probability and i < len(tagged_sentence) - 1:\n",
    "                word1, tag1 = tagged_sentence[i]\n",
    "                word2, tag2 = tagged_sentence[i + 1]\n",
    "\n",
    "                # Randomly select a tag from the two original tags\n",
    "                random_tag = random.choice([tag1, tag2])\n",
    "\n",
    "                joined_word = word1 + word2\n",
    "                modified_sentence.append((joined_word, random_tag))\n",
    "                i += 2  # Move two steps forward\n",
    "                join_count += 1\n",
    "            else:\n",
    "                # Randomly decide whether to split the current word\n",
    "                word, tag = tagged_sentence[i]\n",
    "                if len(word) > 1 and random.random() < split_probability:\n",
    "                    # Split the word into two parts\n",
    "                    split_index = random.randint(1, len(word) - 1)\n",
    "                    part1 = word[:split_index]\n",
    "                    part2 = word[split_index:]\n",
    "                    modified_sentence.append((part1, tag))\n",
    "                    modified_sentence.append((part2, tag))\n",
    "                    split_count += 1\n",
    "                else:\n",
    "                    modified_sentence.append((word, tag))\n",
    "                i += 1\n",
    "\n",
    "        modified_corpus.append(modified_sentence)\n",
    "\n",
    "    return modified_corpus, (split_count, join_count)\n",
    "\n",
    "# Example usage:\n",
    "join_probability = 0  # Adjust this value as desired\n",
    "split_probability = 0  # Adjust this value as desired\n",
    "modified_treebank, (split_count, join_count) = modify_sentence_with_probability_and_count(tagged_sentences, join_probability, split_probability)\n",
    "\n",
    "# Print the counts of splits and joins\n",
    "print(\"Splits:\", split_count)\n",
    "print(\"Joins:\", join_count)\n",
    "\n",
    "# Print the original and modified tagged sentences\n",
    "print(\"Original Sentence:\")\n",
    "print(tagged_sentences[561])\n",
    "print(\"\\nModified Sentence:\")\n",
    "print(modified_treebank[561])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brill function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_brill(data, num_repetitions, joinprob, splitprob):\n",
    "\n",
    "    \"\"\"\n",
    "    Train and evaluate a Brill tagger.\n",
    "\n",
    "    Parameters:\n",
    "    - data: List of feature-label pairs.\n",
    "    - num_repetitions: number of times to repeat the experiment.\n",
    "    - joinprob: probability of randomly joining words\n",
    "    - splitprob: probability of randomly splitting words\n",
    "\n",
    "    Returns:\n",
    "    - f1_scores: list of f1 scores with length equal to num_repetitions.\n",
    "    - split_counts: list of splits made each run with length equal to num_repetitions.\n",
    "    - join_counts: list of joins made each run with length equal to num_repetitions.\n",
    "    \"\"\"\n",
    "    \n",
    "    f1_scores = []\n",
    "    split_counts = []\n",
    "    join_counts = []\n",
    "\n",
    "    for _ in range(num_repetitions):\n",
    "        # train test split\n",
    "        train_set, test_set = train_test_split(data, train_size=0.8)\n",
    "\n",
    "        # corrupt the training set\n",
    "        corrupt_train_set, (split_count, join_count) = modify_sentence_with_probability_and_count(train_set, join_probability=joinprob, split_probability=splitprob)\n",
    "        split_counts.append(split_count)\n",
    "        join_counts.append(join_count)\n",
    "\n",
    "        tag1 = DefaultTagger('NN')\n",
    "        unigram_tagger = UnigramTagger(corrupt_train_set, backoff=tag1)\n",
    "\n",
    "        templates = nltk.brill.nltkdemo18()\n",
    "        trainer = BrillTaggerTrainer(templates=templates, initial_tagger=unigram_tagger)\n",
    "\n",
    "        # Train the Brill Tagger using the templates\n",
    "        brill_tagger = trainer.train(corrupt_train_set, max_rules=200)\n",
    "\n",
    "        # get predictions\n",
    "        test_untagged_words = [tup[0] for sent in test_set for tup in sent]\n",
    "        tags = brill_tagger.tag(test_untagged_words)\n",
    "        brill_preds = [tag for  _,tag in tags]\n",
    "\n",
    "        test_true_tags = [tup[1] for sent in test_set for tup in sent]\n",
    "        f1 = f1_score(brill_preds, test_true_tags, average='weighted')\n",
    "\n",
    "\n",
    "\n",
    "        f1 = f1_score(test_true_tags, brill_preds, average='weighted')\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    return f1_scores, split_counts, join_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unigram function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_unigram(data, num_repetitions, joinprob, splitprob):\n",
    "\n",
    "    \"\"\"\n",
    "    Train and evaluate a Unigram tagger.\n",
    "\n",
    "    Parameters:\n",
    "    - data: List of feature-label pairs.\n",
    "    - num_repetitions: number of times to repeat the experiment.\n",
    "    - joinprob: probability of randomly joining words\n",
    "    - splitprob: probability of randomly splitting words\n",
    "\n",
    "    Returns:\n",
    "    - f1_scores: list of f1 scores with length equal to num_repetitions.\n",
    "    - split_counts: list of splits made each run with length equal to num_repetitions.\n",
    "    - join_counts: list of joins made each run with length equal to num_repetitions.\n",
    "    \"\"\"\n",
    "    \n",
    "    f1_scores = []\n",
    "    split_counts = []\n",
    "    join_counts = []\n",
    "\n",
    "    for _ in range(num_repetitions):\n",
    "        # train test split\n",
    "        train_set, test_set = train_test_split(data, train_size=0.8)\n",
    "\n",
    "        # corrupt train set\n",
    "        corrupt_train_set, (split_count, join_count) = modify_sentence_with_probability_and_count(train_set, join_probability=joinprob, split_probability=splitprob)\n",
    "        split_counts.append(split_count)\n",
    "        join_counts.append(join_count)\n",
    "\n",
    "        tag1 = DefaultTagger('NN')\n",
    "        unigram_tagger = UnigramTagger(corrupt_train_set, backoff=tag1)\n",
    "\n",
    "     \n",
    "        # get predictions\n",
    "        test_untagged_words = [tup[0] for sent in test_set for tup in sent]\n",
    "        unigram_tags = unigram_tagger.tag(test_untagged_words)\n",
    "        unigram_preds = [tag for  _,tag in unigram_tags]\n",
    "\n",
    "        test_true_tags = [tup[1] for sent in test_set for tup in sent]\n",
    "\n",
    "        f1 = f1_score(test_true_tags, unigram_preds, average='weighted')\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    return f1_scores, split_counts, join_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HMM function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_hmm(data, num_repetitions, joinprob, splitprob):  \n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Train and evaluate an HMM tagger.\n",
    "\n",
    "    Parameters:\n",
    "    - data: List of feature-label pairs.\n",
    "    - num_repetitions: number of times to repeat the experiment.\n",
    "    - joinprob: probability of randomly joining words\n",
    "    - splitprob: probability of randomly splitting words\n",
    "\n",
    "    Returns:\n",
    "    - f1_scores: list of f1 scores with length equal to num_repetitions.\n",
    "    - split_counts: list of splits made each run with length equal to num_repetitions.\n",
    "    - join_counts: list of joins made each run with length equal to num_repetitions.\n",
    "    \"\"\"\n",
    "\n",
    "    f1_scores = []\n",
    "    split_counts = []\n",
    "    join_counts = []\n",
    "\n",
    "    for _ in range(num_repetitions):\n",
    "                            \n",
    "        train_set, test_set = train_test_split(data, train_size=0.8)\n",
    "\n",
    "        corrupt_train_set, (split_count, join_count) = modify_sentence_with_probability_and_count(train_set, join_probability=joinprob, split_probability=splitprob)\n",
    "        split_counts.append(split_count)\n",
    "        join_counts.append(join_count)\n",
    "\n",
    "        tagger = nltk.HiddenMarkovModelTagger.train(corrupt_train_set)\n",
    "        \n",
    "        predicted_labels = []\n",
    "        correct_labels = []\n",
    "\n",
    "        for i, sent in enumerate(test_set):\n",
    "            predicted_labels += [tag for _, tag in tagger.tag([word for word, _ in sent])]\n",
    "            correct_labels += [tag for _, tag in sent]\n",
    "\n",
    "        f1 = f1_score(correct_labels, predicted_labels, average='weighted')\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    return f1_scores, split_counts, join_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature extraction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_features(sentence, i):\n",
    "\n",
    "    \"\"\"\n",
    "    Extract features for a given index in a sentence.\n",
    "\n",
    "    Parameters:\n",
    "    - sentence: List of feature-label pairs.\n",
    "    - i: index\n",
    "\n",
    "    Returns:\n",
    "    - features: a dictionary of features on a given index.\n",
    "    \"\"\"\n",
    "        \n",
    "    word = sentence[i][0]\n",
    "    tag = sentence[i][1]\n",
    "    features = {\n",
    "        'word': word,\n",
    "        'is_first': i == 0,  # if the word is the first word\n",
    "        'is_last': i == len(sentence) - 1,  # if the word is the last word\n",
    "        'is_capitalized': word[0].upper() == word[0],\n",
    "        'is_all_caps': word.upper() == word,  # word is in uppercase\n",
    "        'is_all_lower': word.lower() == word,  # word is in lowercase\n",
    "        # prefix of the word\n",
    "        'prefix-1': word[0],\n",
    "        'prefix-2': word[:2],\n",
    "        'prefix-3': word[:3],\n",
    "        # suffix of the word\n",
    "        'suffix-1': word[-1],\n",
    "        'suffix-2': word[-2:],\n",
    "        'suffix-3': word[-3:],\n",
    "        # extracting previous word\n",
    "        'prev_word': '' if i == 0 else sentence[i - 1][0],\n",
    "        # extracting next word\n",
    "        'next_word': '' if i == len(sentence) - 1 else sentence[i + 1][0],\n",
    "        'has_hyphen': '-' in word,  # if word has a hyphen\n",
    "        'is_numeric': word.isdigit(),  # if word is numeric\n",
    "        'capitals_inside': word[1:].lower() != word[1:]\n",
    "    }\n",
    "\n",
    "    # Add previous tag and its previous tag\n",
    "    prev_tag = '' if i == 0 else sentence[i - 1][1]\n",
    "    prev_prev_tag = '' if i < 2 else sentence[i - 2][1]\n",
    "    features['prev_prev_tag'] = f'{prev_prev_tag}_{prev_tag}'\n",
    "\n",
    "    # Add word after the next word\n",
    "    features['next_next_word'] = '' if i > len(sentence) - 3 else sentence[i + 2][0]\n",
    "\n",
    "    # Add word before the previous word\n",
    "    features['prev_prev_word'] = '' if i < 2 else sentence[i - 2][0]\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MEMM tuning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.classify import MaxentClassifier\n",
    "# from nltk.classify.util import apply_features\n",
    "# from nltk.metrics import f1_score\n",
    "# import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def train_and_tune_memm(MEMM_train, max_iter_values, num_folds=5):\n",
    "    \"\"\"\n",
    "    Train and tune a Maximum Entropy Markov Model (MEMM) using cross-validation.\n",
    "\n",
    "    Parameters:\n",
    "    - MEMM_train: List of feature-label pairs for training.\n",
    "    - max_iter_values: List of max_iter values to tune.\n",
    "    - num_folds: Number of folds for cross-validation.\n",
    "\n",
    "    Returns:\n",
    "    - best_max_iter: The best max_iter value found.\n",
    "    - best_f1: The F1 score achieved with the best max_iter value.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize variables to keep track of the best max_iter and its associated F1 score\n",
    "    best_max_iter = None\n",
    "    best_f1 = 0.0\n",
    "\n",
    "    # Define the number of folds for cross-validation\n",
    "    kf = KFold(n_splits=num_folds)\n",
    "\n",
    "    for max_iter in max_iter_values:\n",
    "        f1_scores = []\n",
    "\n",
    "        for train_idx, valid_idx in kf.split(MEMM_train):\n",
    "            train_set = [MEMM_train[i] for i in train_idx]\n",
    "            valid_set = [MEMM_train[i] for i in valid_idx]\n",
    "\n",
    "            maxent_classifier = MaxentClassifier.train(train_set, algorithm='gis', max_iter=max_iter)\n",
    "\n",
    "            valid_features = [features for features, _ in valid_set]\n",
    "            valid_labels = [pos for _, pos in valid_set]\n",
    "\n",
    "            predictions = [maxent_classifier.classify(features) for features in valid_features]\n",
    "\n",
    "            f1 = f1_score(valid_labels, predictions, average='weighted')\n",
    "            f1_scores.append(f1)\n",
    "\n",
    "        # Calculate the average F1 score across folds\n",
    "        avg_f1 = np.mean(f1_scores)\n",
    "\n",
    "        # Check if this max_iter gives a better F1 score than the current best\n",
    "        if avg_f1 > best_f1:\n",
    "            best_max_iter = max_iter\n",
    "            best_f1 = avg_f1\n",
    "\n",
    "    return best_max_iter\n",
    "\n",
    "# Example usage:\n",
    "# best_max_iter, best_f1 = train_and_tune_memm(MEMM_train40, max_iter_values=[5, 10, 15, 20])\n",
    "# print(f\"Best max_iter: {best_max_iter}\")\n",
    "# print(f\"Best F1 Score: {best_f1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CRF tuning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_and_tune_crf_with_cv(X, y, param_grid, n_folds=5):\n",
    "    \n",
    "    \"\"\"\n",
    "    Train and tune a Conditional Random Fields (CRF) Model using cross-validation.\n",
    "\n",
    "    Parameters:\n",
    "    - X: list of extracted features\n",
    "    - y: list of corresponding tags\n",
    "    - param_grid: search grid dictionary.\n",
    "    - n_folds: Number of folds for cross-validation.\n",
    "\n",
    "    Returns:\n",
    "    - best_params: A dictionary of the best parameter values found.\n",
    "    \"\"\"\n",
    "\n",
    "    best_f1 = 0.0\n",
    "    best_params = {}\n",
    "\n",
    "    for max_iter in param_grid['max_iterations']:\n",
    "        for c1 in param_grid['c1']:\n",
    "            for c2 in param_grid['c2']:\n",
    "                f1_scores = []\n",
    "\n",
    "                for fold in range(n_folds):\n",
    "                    # Split data into training and validation sets\n",
    "                    train_indices = [i for i in range(len(X)) if i % n_folds != fold]\n",
    "                    valid_indices = [i for i in range(len(X)) if i % n_folds == fold]\n",
    "\n",
    "                    X_train_fold = [X[i] for i in train_indices]\n",
    "                    y_train_fold = [y[i] for i in train_indices]\n",
    "                    X_valid_fold = [X[i] for i in valid_indices]\n",
    "                    y_valid_fold = [y[i] for i in valid_indices]\n",
    "\n",
    "                    # Train the CRF model\n",
    "                    trainer = pycrfsuite.Trainer(verbose=False)\n",
    "                    for x_train, y_train in zip(X_train_fold, y_train_fold):\n",
    "                        trainer.append(x_train, y_train)\n",
    "                    trainer.set_params({\n",
    "                        'max_iterations': max_iter,\n",
    "                        'c1': c1,\n",
    "                        'c2': c2,\n",
    "                        'feature.possible_transitions': True\n",
    "                    })\n",
    "                    trainer.train('temp_model.crfsuite')\n",
    "\n",
    "                    # Test the CRF model\n",
    "                    tagger = pycrfsuite.Tagger()\n",
    "                    tagger.open('temp_model.crfsuite')\n",
    "\n",
    "                    CRF_predictions = [tagger.tag(instance) for instance in X_valid_fold]\n",
    "\n",
    "                    CRF_flat_predictions = [tag for instance_tags in CRF_predictions for tag in instance_tags]\n",
    "                    CRF_flat_ground_truth = [tag for instance_tags in y_valid_fold for tag in instance_tags]\n",
    "\n",
    "                    f1 = f1_score(CRF_flat_ground_truth, CRF_flat_predictions, average='weighted')\n",
    "                    f1_scores.append(f1)\n",
    "\n",
    "                mean_f1 = np.mean(f1_scores)\n",
    "                if mean_f1 > best_f1:\n",
    "                    best_f1 = mean_f1\n",
    "                    best_params = {\n",
    "                        'max_iterations': max_iter,\n",
    "                        'c1': c1,\n",
    "                        'c2': c2,\n",
    "                        'feature.possible_transitions': True\n",
    "                    }\n",
    "\n",
    "    #print(\"Best Parameters: \", best_params)\n",
    "    #print(\"Best F1 Score: \", best_f1)\n",
    "\n",
    "    return best_params\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation for MEMM and CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation\n",
    "\n",
    "random.seed(1234)\n",
    "valid_set, model_set = train_test_split(tagged_sentences, train_size=0.1)\n",
    "\n",
    "# feature extraction    \n",
    "X_valid = []\n",
    "y_valid = []\n",
    "for sentence in valid_set:\n",
    "    X_sentence = []\n",
    "    y_sentence = []\n",
    "    for i in range(len(sentence)):\n",
    "        X_sentence.append(word_features(sentence, i))\n",
    "        y_sentence.append(sentence[i][1])\n",
    "    X_valid.append(X_sentence)\n",
    "    y_valid.append(y_sentence)    \n",
    "\n",
    "MEMM_valid = []  # Collect feature-label pairs for MEMM\n",
    "for sentence_features, sentence_labels in zip(X_valid, y_valid):\n",
    "    MEMM_valid.extend(list(zip(sentence_features, sentence_labels)))  \n",
    "\n",
    "best_max_iter = train_and_tune_memm(MEMM_valid, max_iter_values=[10,20,30])   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the parameter grid for tuning crf\n",
    "param_grid = {\n",
    "    'max_iterations': [20, 50, 100],\n",
    "    'c1': [0.01, 0.1, 1.0, 10],\n",
    "    'c2': [1e-4, 1e-3, 1e-2, 1e-1]\n",
    "}\n",
    "\n",
    "# feature extraction    \n",
    "X_valid = []\n",
    "y_valid = []\n",
    "for sentence in valid_set:\n",
    "    X_sentence = []\n",
    "    y_sentence = []\n",
    "    for i in range(len(sentence)):\n",
    "        X_sentence.append(word_features(sentence, i))\n",
    "        y_sentence.append(sentence[i][1])\n",
    "    X_valid.append(X_sentence)\n",
    "    y_valid.append(y_sentence)    \n",
    "\n",
    "\n",
    "# validation\n",
    "best_param = train_and_tune_crf_with_cv(X_valid, y_valid, param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MEMM function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_memm(data, num_repetitions, joinprob, splitprob, max_iters):\n",
    "\n",
    "    \"\"\"\n",
    "    Train and evaluate an MEMM tagger.\n",
    "\n",
    "    Parameters:\n",
    "    - data: List of feature-label pairs.\n",
    "    - num_repetitions: number of times to repeat the experiment.\n",
    "    - joinprob: probability of randomly joining words\n",
    "    - splitprob: probability of randomly splitting words\n",
    "    - max_iters: maximum iterations for the MaxentClassifier \n",
    "\n",
    "    Returns:\n",
    "    - f1_scores: list of f1 scores with length equal to num_repetitions.\n",
    "    - split_counts: list of splits made each run with length equal to num_repetitions.\n",
    "    - join_counts: list of joins made each run with length equal to num_repetitions.\n",
    "    \"\"\"\n",
    "   \n",
    "    # initialize list to keep track of model performance\n",
    "    f1_scores = []\n",
    "    split_counts = []\n",
    "    join_counts = []\n",
    "    \n",
    "\n",
    "    for _ in range(num_repetitions):\n",
    "\n",
    "        # train test split and formatting\n",
    "        train_set, test_set = train_test_split(data, train_size=0.8)\n",
    "\n",
    "    \n",
    "        corrupt_train_set, (split_count, join_count) = modify_sentence_with_probability_and_count(train_set, join_probability=joinprob, split_probability=splitprob)\n",
    "        split_counts.append(split_count)\n",
    "        join_counts.append(join_count)\n",
    "\n",
    "        # feature extraction    \n",
    "        X_train = []\n",
    "        y_train = []\n",
    "        for sentence in corrupt_train_set:\n",
    "            X_sentence = []\n",
    "            y_sentence = []\n",
    "            for i in range(len(sentence)):\n",
    "                X_sentence.append(word_features(sentence, i))\n",
    "                y_sentence.append(sentence[i][1])\n",
    "            X_train.append(X_sentence)\n",
    "            y_train.append(y_sentence) \n",
    "\n",
    "\n",
    "        X_test = []\n",
    "        y_test = []\n",
    "        for sentence in test_set:\n",
    "            X_sentence = []\n",
    "            y_sentence = []\n",
    "            for i in range(len(sentence)):\n",
    "                X_sentence.append(word_features(sentence, i))\n",
    "                y_sentence.append(sentence[i][1])\n",
    "            X_test.append(X_sentence)\n",
    "            y_test.append(y_sentence) \n",
    "        \n",
    "        \n",
    "        MEMM_train = []  # Collect feature-label pairs for MEMM\n",
    "        for sentence_features, sentence_labels in zip(X_train, y_train):\n",
    "            MEMM_train.extend(list(zip(sentence_features, sentence_labels)))\n",
    "\n",
    "        MEMM_test = []  # Collect feature-label pairs for MEMM\n",
    "        for sentence_features, sentence_labels in zip(X_test, y_test):\n",
    "            MEMM_test.extend(list(zip(sentence_features, sentence_labels)))\n",
    "\n",
    "        \n",
    "        # training using the tuned value\n",
    "        maxent_classifier = MaxentClassifier.train(MEMM_train, algorithm='gis', max_iter=max_iters)\n",
    "\n",
    "        # predictions \n",
    "        memm_predictions = maxent_classifier.classify_many([features for features, _ in MEMM_test])\n",
    "\n",
    "        memm_true_labels = [pos for _, pos in MEMM_test]\n",
    "        \n",
    "        \n",
    "        f1 = f1_score(memm_true_labels, memm_predictions, average='weighted')\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "\n",
    "    return f1_scores, split_counts, join_counts\n",
    "\n",
    "\n",
    "#train_and_evaluate_memm(Xdata=X, ydata=y, num_repetitions=2, train_prop=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CRF function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_and_evaluate_crf(data, num_repetitions, joinprob, splitprob, param_grid):\n",
    "\n",
    "    \"\"\"\n",
    "    Train and evaluate a CRF tagger.\n",
    "\n",
    "    Parameters:\n",
    "    - data: List of feature-label pairs.\n",
    "    - num_repetitions: number of times to repeat the experiment.\n",
    "    - joinprob: probability of randomly joining words\n",
    "    - splitprob: probability of randomly splitting words\n",
    "    - param_grid: dictionary of parameter specifications for the maximum number of iterations and regularization parameters\n",
    "\n",
    "    Returns:\n",
    "    - f1_scores: list of f1 scores with length equal to num_repetitions.\n",
    "    - split_counts: list of splits made each run with length equal to num_repetitions.\n",
    "    - join_counts: list of joins made each run with length equal to num_repetitions.\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize list to keep track of model performance\n",
    "    f1_scores = []\n",
    "    split_counts = []\n",
    "    join_counts = []\n",
    "\n",
    "\n",
    "\n",
    "    for _ in range(num_repetitions):\n",
    "\n",
    "         # train test split and formatting\n",
    "        train_set, test_set = train_test_split(data, train_size=0.8)\n",
    "\n",
    "        \n",
    "        corrupt_train_set, (split_count, join_count) = modify_sentence_with_probability_and_count(train_set, join_probability=joinprob, split_probability=splitprob)\n",
    "        split_counts.append(split_count)\n",
    "        join_counts.append(join_count)\n",
    "\n",
    "        # feature extraction    \n",
    "        X_train = []\n",
    "        y_train = []\n",
    "        for sentence in corrupt_train_set:\n",
    "            X_sentence = []\n",
    "            y_sentence = []\n",
    "            for i in range(len(sentence)):\n",
    "                X_sentence.append(word_features(sentence, i))\n",
    "                y_sentence.append(sentence[i][1])\n",
    "            X_train.append(X_sentence)\n",
    "            y_train.append(y_sentence) \n",
    "\n",
    "\n",
    "        X_test = []\n",
    "        y_test = []\n",
    "        for sentence in test_set:\n",
    "            X_sentence = []\n",
    "            y_sentence = []\n",
    "            for i in range(len(sentence)):\n",
    "                X_sentence.append(word_features(sentence, i))\n",
    "                y_sentence.append(sentence[i][1])\n",
    "            X_test.append(X_sentence)\n",
    "            y_test.append(y_sentence)    \n",
    "   \n",
    "\n",
    "        \n",
    "\n",
    "        # training using the tuned value\n",
    "        trainer = pycrfsuite.Trainer(verbose=False)\n",
    "\n",
    "        # Add training data\n",
    "        for x, y in zip(X_train, y_train):\n",
    "\t        trainer.append(x, y)\n",
    "\n",
    "        # Set trainer parameters\n",
    "        trainer.set_params(param_grid)\n",
    "\n",
    "        # Train the CRF model\n",
    "        trainer.train('pos.crfsuite')\n",
    "\n",
    "\n",
    "        # Testing\n",
    "        # Initialize the tagger\n",
    "        tagger = pycrfsuite.Tagger()\n",
    "        tagger.open('pos.crfsuite')\n",
    "\n",
    "\n",
    "        # predictions\n",
    "        CRF_predictions = [tagger.tag(instance) for instance in X_test]\n",
    "\n",
    "        CRF_flat_predictions = [tag for instance_tags in CRF_predictions for tag in instance_tags]\n",
    "        CRF_flat_ground_truth = [tag for instance_tags in y_test for tag in instance_tags]\n",
    "\n",
    "        # test score\n",
    "        f1 = f1_score(CRF_flat_ground_truth, CRF_flat_predictions, average='weighted')\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "\n",
    "    return f1_scores, split_counts, join_counts\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collecting all results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random.seed(1234)\n",
    "\n",
    "# unigram_clean, unigram_clean_splits, unigram_clean_joins = train_and_evaluate_unigram(data=model_set, num_repetitions=5, joinprob=0,splitprob=0)\n",
    "# unigram_low, unigram_low_splits, unigram_low_joins = train_and_evaluate_unigram(data=model_set, num_repetitions=5, joinprob=0.1,splitprob=0.05)\n",
    "# unigram_mid, unigram_mid_splits, unigram_mid_joins = train_and_evaluate_unigram(data=model_set, num_repetitions=5, joinprob=0.3,splitprob=0.11)\n",
    "# unigram_high, unigram_high_splits, unigram_high_joins = train_and_evaluate_unigram(data=model_set, num_repetitions=5, joinprob=0.6,splitprob=0.33)\n",
    "\n",
    "# brill_clean, brill_clean_splits, brill_clean_joins = train_and_evaluate_brill(data=model_set, num_repetitions=5, joinprob=0,splitprob=0)\n",
    "# brill_low, brill_low_splits, brill_low_joins = train_and_evaluate_brill(data=model_set, num_repetitions=5, joinprob=0.1,splitprob=0.05)\n",
    "# brill_mid, brill_mid_splits, brill_mid_joins = train_and_evaluate_brill(data=model_set, num_repetitions=5, joinprob=0.3,splitprob=0.11)\n",
    "# brill_high, brill_high_splits, brill_high_joins = train_and_evaluate_brill(data=model_set, num_repetitions=5, joinprob=0.6,splitprob=0.33)\n",
    "\n",
    "\n",
    "# hmm_clean, hmm_clean_splits, hmm_clean_joins = train_and_evaluate_hmm(data=model_set, num_repetitions=5, joinprob=0,splitprob=0)\n",
    "# hmm_low, hmm_low_splits, hmm_low_joins = train_and_evaluate_hmm(data=model_set, num_repetitions=5, joinprob=0.1,splitprob=0.05)\n",
    "# hmm_mid, hmm_mid_splits, hmm_mid_joins = train_and_evaluate_hmm(data=model_set, num_repetitions=5, joinprob=0.3,splitprob=0.11)\n",
    "# hmm_high, hmm_high_splits, hmm_high_joins = train_and_evaluate_hmm(data=model_set, num_repetitions=5, joinprob=0.6,splitprob=0.33)\n",
    "\n",
    "# memm_clean, memm_clean_splits, memm_clean_joins = train_and_evaluate_memm(data=model_set, num_repetitions=5, joinprob=0,splitprob=0, max_iters=best_max_iter)\n",
    "# memm_low, memm_low_splits, memm_low_joins = train_and_evaluate_memm(data=model_set, num_repetitions=5, joinprob=0.1,splitprob=0.05, max_iters=best_max_iter)\n",
    "# memm_mid, memm_mid_splits, memm_mid_joins = train_and_evaluate_memm(data=model_set, num_repetitions=5, joinprob=0.3,splitprob=0.11, max_iters=best_max_iter)\n",
    "# memm_high, memm_high_splits, memm_high_joins = train_and_evaluate_memm(data=model_set, num_repetitions=5, joinprob=0.6,splitprob=0.33, max_iters=best_max_iter)\n",
    "\n",
    "# crf_clean, crf_clean_splits, crf_clean_joins = train_and_evaluate_crf(data=model_set, num_repetitions=5, joinprob=0,splitprob=0, param_grid=best_param)\n",
    "# crf_low, crf_low_splits, crf_low_joins = train_and_evaluate_crf(data=model_set, num_repetitions=5, joinprob=0.1,splitprob=0.05, param_grid=best_param)\n",
    "# crf_mid, crf_mid_splits, crf_mid_joins = train_and_evaluate_crf(data=model_set, num_repetitions=5, joinprob=0.3,splitprob=0.11, param_grid=best_param)\n",
    "# crf_high, crf_high_splits, crf_high_joins = train_and_evaluate_crf(data=model_set, num_repetitions=5, joinprob=0.6,splitprob=0.33, param_grid=best_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1_scores = {\n",
    "#     \"hmm_clean\": hmm_clean,\n",
    "#     \"hmm_low\": hmm_low,\n",
    "#     \"hmm_mid\": hmm_mid,\n",
    "#     \"hmm_high\": hmm_high,\n",
    "#     \"memm_clean\": memm_clean,\n",
    "#     \"memm_low\": memm_low,\n",
    "#     \"memm_mid\": memm_mid,\n",
    "#     \"memm_high\": memm_high,\n",
    "#     \"crf_clean\": crf_clean,\n",
    "#     \"crf_low\": crf_low,\n",
    "#     \"crf_mid\": crf_mid,\n",
    "#     \"crf_high\": crf_high,\n",
    "#     \"unigram_clean\": unigram_clean,\n",
    "#     \"unigram_low\": unigram_low,\n",
    "#     \"unigram_mid\": unigram_mid,\n",
    "#     \"unigram_high\": unigram_high,\n",
    "#     \"brill_clean\": brill_clean,\n",
    "#     \"brill_low\": brill_low,\n",
    "#     \"brill_mid\": brill_mid,\n",
    "#     \"brill_high\": brill_high\n",
    "# }\n",
    "\n",
    "# with open('insert file path here', \"w\") as file:\n",
    "#     for model_threshold, f1_score in f1_scores.items():\n",
    "#         file.write(f\"{model_threshold}: {f1_score}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('insert file path', \"w\") as file:\n",
    "#     for param, paramval in best_param.items():\n",
    "#         file.write(f\"{param}: {paramval}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_it = {\n",
    "#     \"max_it\": best_max_iter\n",
    "# }\n",
    "\n",
    "# with open('insert file path here', \"w\") as file:\n",
    "#     for param, paramval in max_it.items():\n",
    "#         file.write(f\"{param}: {paramval}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
