{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#from data_loader import load_word_dataset\n",
    "import random\n",
    "import numpy as np\n",
    "import sys\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "import string\n",
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import pprint, time\n",
    "\n",
    "\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "from nltk.corpus import treebank\n",
    "from nltk.corpus import brown\n",
    "\n",
    "from nltk.classify import MaxentClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "import pycrfsuite\n",
    "from nltk.tag import hmm\n",
    "# from nltk.classify import megam\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from nltk.tag import BrillTaggerTrainer\n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.tag import DefaultTagger\n",
    "from nltk.metrics import ConfusionMatrix\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\kstap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#download the treebank corpus from nltk\n",
    "\n",
    "nltk.download('treebank')\n",
    "  \n",
    "# reading the Treebank tagged sentences\n",
    "tagged_sentences = list(nltk.corpus.treebank.tagged_sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_words = [ tup for sent in tagged_sentences for tup in sent ]\n",
    "#use set datatype to check how many unique tags are present in training data\n",
    "tags = {tag for word,tag in tagged_words}\n",
    "print(len(tags))\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to corrupt test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def replace_letters_in_word(word, probability):\n",
    "\n",
    "    \"\"\"\n",
    "    Corrupt a word by randomly switching letters with a given probability.\n",
    "\n",
    "    Args:\n",
    "    word: correctly spelled\n",
    "    probability: The probability condition.\n",
    "\n",
    "    Returns:\n",
    "    modified_word: mispelled word\n",
    "    \"\"\"\n",
    "        \n",
    "    modified_word = ''\n",
    "    for char in word:\n",
    "        if char.isalpha() and random.uniform(0, 1) < probability:\n",
    "            # Replace only if the character is a letter and meets the probability condition\n",
    "            modified_word += chr(random.randint(97, 122))  # ASCII values for lowercase letters\n",
    "        else:\n",
    "            modified_word += char\n",
    "    return modified_word\n",
    "\n",
    "\n",
    "def modify_sentences(sentences, probability):\n",
    "\n",
    "    \"\"\"\n",
    "    Corrupt a sentence by mispelling words.\n",
    "\n",
    "    Args:\n",
    "    sentences: input sentence\n",
    "    probability: probability condition, passed to the replace_letters_in_word function\n",
    "\n",
    "    Returns:\n",
    "    modified_sentences: corrupted sentence\n",
    "    \"\"\"\n",
    "    modified_sentences = []\n",
    "\n",
    "    for tagged_sentence in sentences:\n",
    "        modified_sentence = []\n",
    "        for word, tag in tagged_sentence:\n",
    "            modified_word = replace_letters_in_word(word, probability)\n",
    "            modified_sentence.append((modified_word, tag))\n",
    "        modified_sentences.append(modified_sentence)\n",
    "\n",
    "    return modified_sentences\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: [('Pierre', 'NOUN'), ('Vinken', 'NOUN'), (',', '.'), ('61', 'NUM'), ('years', 'NOUN'), ('old', 'ADJ'), (',', '.'), ('will', 'VERB'), ('join', 'VERB'), ('the', 'DET'), ('board', 'NOUN'), ('as', 'ADP'), ('a', 'DET'), ('nonexecutive', 'ADJ'), ('director', 'NOUN'), ('Nov.', 'NOUN'), ('29', 'NUM'), ('.', '.')]\n",
      "Modified Sentence: [('rierce', 'NOUN'), ('aiakew', 'NOUN'), (',', '.'), ('61', 'NUM'), ('years', 'NOUN'), ('old', 'ADJ'), (',', '.'), ('will', 'VERB'), ('nqrc', 'VERB'), ('tfu', 'DET'), ('woarr', 'NOUN'), ('as', 'ADP'), ('l', 'DET'), ('nonexecutifn', 'ADJ'), ('digechow', 'NOUN'), ('Nov.', 'NOUN'), ('29', 'NUM'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage\n",
    "original_sentence = tagged_sentences[0]\n",
    "modified_sentence = modify_sentences([original_sentence], 0.35)[0]\n",
    "\n",
    "print(\"Original Sentence:\", original_sentence)\n",
    "print(\"Modified Sentence:\", modified_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brill function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_brill(data, num_repetitions, train_prop, prob):\n",
    "\n",
    "    \"\"\"\n",
    "    Train and evaluate a Brill tagger.\n",
    "\n",
    "    Parameters:\n",
    "    - data: List of feature-label pairs.\n",
    "    - num_repetitions: number of times to repeat the experiment.\n",
    "    - train_prop: proportion of training data.\n",
    "    - prob: probability of randomly switching letters, passed to the modify_sentences function\n",
    "\n",
    "    Returns:\n",
    "    - f1_scores: list of f1 scores with length equal to num_repetitions.\n",
    "    \"\"\"\n",
    "\n",
    "    f1_scores = []\n",
    "  \n",
    "    for _ in range(num_repetitions):\n",
    "        # train test split\n",
    "        train_set, test_set = train_test_split(data, train_size=train_prop)\n",
    "\n",
    "        # corrupt the test set\n",
    "        corrupt_test_set = modify_sentences(test_set, probability=prob)\n",
    "    \n",
    "        tag1 = DefaultTagger('NN')\n",
    "        unigram_tagger = UnigramTagger(train_set, backoff=tag1)\n",
    "\n",
    "        templates = nltk.brill.nltkdemo18()\n",
    "        trainer = BrillTaggerTrainer(templates=templates, initial_tagger=unigram_tagger)\n",
    "\n",
    "        # Train the Brill Tagger using the templates\n",
    "        brill_tagger = trainer.train(train_set, max_rules=200)\n",
    "\n",
    "        # get predictions\n",
    "        test_untagged_words = [tup[0] for sent in corrupt_test_set for tup in sent]\n",
    "        tags = brill_tagger.tag(test_untagged_words)\n",
    "        brill_preds = [tag for  _,tag in tags]\n",
    "\n",
    "        test_true_tags = [tup[1] for sent in corrupt_test_set for tup in sent]\n",
    "        f1 = f1_score(brill_preds, test_true_tags, average='weighted')\n",
    "\n",
    "\n",
    "\n",
    "        f1 = f1_score(test_true_tags, brill_preds, average='weighted')\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    return f1_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unigram function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_unigram(data, num_repetitions, train_prop, prob):\n",
    "\n",
    "    \"\"\"\n",
    "    Train and evaluate a Uingram tagger.\n",
    "\n",
    "    Parameters:\n",
    "    - data: List of feature-label pairs.\n",
    "    - num_repetitions: number of times to repeat the experiment.\n",
    "    - train_prop: proportion of training data.\n",
    "    - prob: probability of randomly switching letters, passed to the modify_sentences function\n",
    "\n",
    "    Returns:\n",
    "    - f1_scores: list of f1 scores with length equal to num_repetitions.\n",
    "    \"\"\"\n",
    "\n",
    "    f1_scores = []\n",
    "\n",
    "    for _ in range(num_repetitions):\n",
    "        # train test split\n",
    "        train_set, test_set = train_test_split(data, train_size=train_prop)\n",
    "\n",
    "        # corrupt test set\n",
    "        corrupt_test_set = modify_sentences(test_set, probability=prob)\n",
    "\n",
    "        tag1 = DefaultTagger('NN')\n",
    "        unigram_tagger = UnigramTagger(train_set, backoff=tag1)\n",
    "\n",
    "     \n",
    "        # get predictions\n",
    "        test_untagged_words = [tup[0] for sent in corrupt_test_set for tup in sent]\n",
    "        unigram_tags = unigram_tagger.tag(test_untagged_words)\n",
    "        unigram_preds = [tag for  _,tag in unigram_tags]\n",
    "\n",
    "        test_true_tags = [tup[1] for sent in corrupt_test_set for tup in sent]\n",
    "\n",
    "        f1 = f1_score(test_true_tags, unigram_preds, average='weighted')\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    return f1_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HMM function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_hmm(data, num_repetitions, prob):  \n",
    "    f1_scores = []\n",
    "\n",
    "    \"\"\"\n",
    "    Train and evaluate an HMM tagger.\n",
    "\n",
    "    Parameters:\n",
    "    - data: List of feature-label pairs.\n",
    "    - num_repetitions: number of times to repeat the experiment.\n",
    "    - train_prop: proportion of training data.\n",
    "    - prob: probability of randomly switching letters, passed to the modify_sentences function\n",
    "\n",
    "    Returns:\n",
    "    - f1_scores: list of f1 scores with length equal to num_repetitions.\n",
    "    \"\"\"\n",
    "\n",
    "    for _ in range(num_repetitions):\n",
    "                            \n",
    "        train_set, test_set = train_test_split(data, train_size=0.8)\n",
    "\n",
    "        # corrupt test set\n",
    "        corrupt_test_set = modify_sentences(test_set, probability=prob)\n",
    "\n",
    "        tagger = nltk.HiddenMarkovModelTagger.train(train_set)\n",
    "        \n",
    "        predicted_labels = []\n",
    "        correct_labels = []\n",
    "\n",
    "        for i, sent in enumerate(corrupt_test_set):\n",
    "            predicted_labels += [tag for _, tag in tagger.tag([word for word, _ in sent])]\n",
    "            correct_labels += [tag for _, tag in sent]\n",
    "\n",
    "        f1 = f1_score(correct_labels, predicted_labels, average='weighted')\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    return f1_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature extraction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_features(sentence, i):\n",
    "    \n",
    "    \"\"\"\n",
    "    Extract features for a given index in a sentence.\n",
    "\n",
    "    Parameters:\n",
    "    - sentence: List of feature-label pairs.\n",
    "    - i: index\n",
    "\n",
    "    Returns:\n",
    "    - features: a dictionary of features on a given index.\n",
    "    \"\"\"\n",
    "\n",
    "    word = sentence[i][0]\n",
    "    tag = sentence[i][1]\n",
    "    features = {\n",
    "        'word': word,\n",
    "        'is_first': i == 0,  # if the word is the first word\n",
    "        'is_last': i == len(sentence) - 1,  # if the word is the last word\n",
    "        'is_capitalized': word[0].upper() == word[0],\n",
    "        'is_all_caps': word.upper() == word,  # word is in uppercase\n",
    "        'is_all_lower': word.lower() == word,  # word is in lowercase\n",
    "        # prefix of the word\n",
    "        'prefix-1': word[0],\n",
    "        'prefix-2': word[:2],\n",
    "        'prefix-3': word[:3],\n",
    "        # suffix of the word\n",
    "        'suffix-1': word[-1],\n",
    "        'suffix-2': word[-2:],\n",
    "        'suffix-3': word[-3:],\n",
    "        # extracting previous word\n",
    "        'prev_word': '' if i == 0 else sentence[i - 1][0],\n",
    "        # extracting next word\n",
    "        'next_word': '' if i == len(sentence) - 1 else sentence[i + 1][0],\n",
    "        'has_hyphen': '-' in word,  # if word has a hyphen\n",
    "        'is_numeric': word.isdigit(),  # if word is numeric\n",
    "        'capitals_inside': word[1:].lower() != word[1:]\n",
    "    }\n",
    "\n",
    "    # Add previous tag and its previous tag\n",
    "    prev_tag = '' if i == 0 else sentence[i - 1][1]\n",
    "    prev_prev_tag = '' if i < 2 else sentence[i - 2][1]\n",
    "    features['prev_prev_tag'] = f'{prev_prev_tag}_{prev_tag}'\n",
    "\n",
    "    # Add word after the next word\n",
    "    features['next_next_word'] = '' if i > len(sentence) - 3 else sentence[i + 2][0]\n",
    "\n",
    "    # Add word before the previous word\n",
    "    features['prev_prev_word'] = '' if i < 2 else sentence[i - 2][0]\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MEMM tuning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_tune_memm(MEMM_train, max_iter_values, num_folds=5):\n",
    "    \"\"\"\n",
    "    Train and tune a Maximum Entropy Markov Model (MEMM) using cross-validation.\n",
    "\n",
    "    Parameters:\n",
    "    - MEMM_train: List of feature-label pairs for training.\n",
    "    - max_iter_values: List of max_iter values to tune.\n",
    "    - num_folds: Number of folds for cross-validation.\n",
    "\n",
    "    Returns:\n",
    "    - best_max_iter: The best max_iter value found.\n",
    "    - best_f1: The F1 score achieved with the best max_iter value.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize variables to keep track of the best max_iter and its associated F1 score\n",
    "    best_max_iter = None\n",
    "    best_f1 = 0.0\n",
    "\n",
    "    # Define the number of folds for cross-validation\n",
    "    kf = KFold(n_splits=num_folds)\n",
    "\n",
    "    for max_iter in max_iter_values:\n",
    "        f1_scores = []\n",
    "\n",
    "        for train_idx, valid_idx in kf.split(MEMM_train):\n",
    "            train_set = [MEMM_train[i] for i in train_idx]\n",
    "            valid_set = [MEMM_train[i] for i in valid_idx]\n",
    "\n",
    "            maxent_classifier = MaxentClassifier.train(train_set, algorithm='gis', max_iter=max_iter)\n",
    "\n",
    "            valid_features = [features for features, _ in valid_set]\n",
    "            valid_labels = [pos for _, pos in valid_set]\n",
    "\n",
    "            predictions = [maxent_classifier.classify(features) for features in valid_features]\n",
    "\n",
    "            f1 = f1_score(valid_labels, predictions, average='weighted')\n",
    "            f1_scores.append(f1)\n",
    "\n",
    "        # Calculate the average F1 score across folds\n",
    "        avg_f1 = np.mean(f1_scores)\n",
    "\n",
    "        # Check if this max_iter gives a better F1 score than the current best\n",
    "        if avg_f1 > best_f1:\n",
    "            best_max_iter = max_iter\n",
    "            best_f1 = avg_f1\n",
    "\n",
    "    return best_max_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CRF tuning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_and_tune_crf_with_cv(X, y, param_grid, n_folds=5):\n",
    "    \"\"\"\n",
    "    Train and tune a Conditional Random Fields (CRF) Model using cross-validation.\n",
    "\n",
    "    Parameters:\n",
    "    - X: list of extracted features\n",
    "    - y: list of corresponding tags\n",
    "    - param_grid: search grid dictionary.\n",
    "    - n_folds: Number of folds for cross-validation.\n",
    "\n",
    "    Returns:\n",
    "    - best_params: A dictionary of the best parameter values found.\n",
    "    \"\"\"\n",
    "\n",
    "    best_f1 = 0.0\n",
    "    best_params = {}\n",
    "\n",
    "    for max_iter in param_grid['max_iterations']:\n",
    "        for c1 in param_grid['c1']:\n",
    "            for c2 in param_grid['c2']:\n",
    "                f1_scores = []\n",
    "\n",
    "                for fold in range(n_folds):\n",
    "                    # Split data into training and validation sets\n",
    "                    train_indices = [i for i in range(len(X)) if i % n_folds != fold]\n",
    "                    valid_indices = [i for i in range(len(X)) if i % n_folds == fold]\n",
    "\n",
    "                    X_train_fold = [X[i] for i in train_indices]\n",
    "                    y_train_fold = [y[i] for i in train_indices]\n",
    "                    X_valid_fold = [X[i] for i in valid_indices]\n",
    "                    y_valid_fold = [y[i] for i in valid_indices]\n",
    "\n",
    "                    # Train the CRF model\n",
    "                    trainer = pycrfsuite.Trainer(verbose=False)\n",
    "                    for x_train, y_train in zip(X_train_fold, y_train_fold):\n",
    "                        trainer.append(x_train, y_train)\n",
    "                    trainer.set_params({\n",
    "                        'max_iterations': max_iter,\n",
    "                        'c1': c1,\n",
    "                        'c2': c2,\n",
    "                        'feature.possible_transitions': True\n",
    "                    })\n",
    "                    trainer.train('temp_model.crfsuite')\n",
    "\n",
    "                    # Test the CRF model\n",
    "                    tagger = pycrfsuite.Tagger()\n",
    "                    tagger.open('temp_model.crfsuite')\n",
    "\n",
    "                    CRF_predictions = [tagger.tag(instance) for instance in X_valid_fold]\n",
    "\n",
    "                    CRF_flat_predictions = [tag for instance_tags in CRF_predictions for tag in instance_tags]\n",
    "                    CRF_flat_ground_truth = [tag for instance_tags in y_valid_fold for tag in instance_tags]\n",
    "\n",
    "                    f1 = f1_score(CRF_flat_ground_truth, CRF_flat_predictions, average='weighted')\n",
    "                    f1_scores.append(f1)\n",
    "\n",
    "                mean_f1 = np.mean(f1_scores)\n",
    "                if mean_f1 > best_f1:\n",
    "                    best_f1 = mean_f1\n",
    "                    best_params = {\n",
    "                        'max_iterations': max_iter,\n",
    "                        'c1': c1,\n",
    "                        'c2': c2,\n",
    "                        'feature.possible_transitions': True\n",
    "                    }\n",
    "\n",
    "    #print(\"Best Parameters: \", best_params)\n",
    "    #print(\"Best F1 Score: \", best_f1)\n",
    "\n",
    "    return best_params\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation for MEMM and CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation\n",
    "\n",
    "random.seed(1234)\n",
    "valid_set, model_set = train_test_split(tagged_sentences, train_size=0.1, random_state=1234)\n",
    "\n",
    "# feature extraction    \n",
    "X_valid = []\n",
    "y_valid = []\n",
    "for sentence in valid_set:\n",
    "    X_sentence = []\n",
    "    y_sentence = []\n",
    "    for i in range(len(sentence)):\n",
    "        X_sentence.append(word_features(sentence, i))\n",
    "        y_sentence.append(sentence[i][1])\n",
    "    X_valid.append(X_sentence)\n",
    "    y_valid.append(y_sentence)    \n",
    "\n",
    "MEMM_valid = []  # Collect feature-label pairs for MEMM\n",
    "for sentence_features, sentence_labels in zip(X_valid, y_valid):\n",
    "    MEMM_valid.extend(list(zip(sentence_features, sentence_labels)))  \n",
    "\n",
    "best_max_iter = train_and_tune_memm(MEMM_valid, max_iter_values=[10,20,30])   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the parameter grid for tuning crf\n",
    "param_grid = {\n",
    "    'max_iterations': [20, 50, 100],\n",
    "    'c1': [0.01, 0.1, 1.0, 10],\n",
    "    'c2': [1e-4, 1e-3, 1e-2, 1e-1]\n",
    "}\n",
    "\n",
    "# # feature extraction    \n",
    "X_valid = []\n",
    "y_valid = []\n",
    "for sentence in valid_set:\n",
    "    X_sentence = []\n",
    "    y_sentence = []\n",
    "    for i in range(len(sentence)):\n",
    "        X_sentence.append(word_features(sentence, i))\n",
    "        y_sentence.append(sentence[i][1])\n",
    "    X_valid.append(X_sentence)\n",
    "    y_valid.append(y_sentence)    \n",
    "\n",
    "\n",
    "# validation\n",
    "best_param = train_and_tune_crf_with_cv(X_valid, y_valid, param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MEMM function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_memm(data, num_repetitions,prob, max_iters):\n",
    "\n",
    "    \"\"\"\n",
    "    Train and evaluate an MEMM tagger.\n",
    "\n",
    "    Parameters:\n",
    "    - data: List of feature-label pairs.\n",
    "    - num_repetitions: number of times to repeat the experiment.\n",
    "    - prob: probability of randomly switching letters, passed to the modify_sentences function\n",
    "    - max_iters: maximum iterations for the MaxentClassifier \n",
    "\n",
    "    Returns:\n",
    "    - f1_scores: list of f1 scores with length equal to num_repetitions.\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize list to keep track of model performance\n",
    "    f1_scores = []\n",
    "    \n",
    "\n",
    "    for _ in range(num_repetitions):\n",
    "\n",
    "        # train test split and formatting\n",
    "        train_set, test_set = train_test_split(data, train_size=0.8)\n",
    "\n",
    "    \n",
    "        # corrupt test set\n",
    "        corrupt_test_set = modify_sentences(test_set, probability=prob)\n",
    "\n",
    "        # feature extraction    \n",
    "        X_train = []\n",
    "        y_train = []\n",
    "        for sentence in train_set:\n",
    "            X_sentence = []\n",
    "            y_sentence = []\n",
    "            for i in range(len(sentence)):\n",
    "                X_sentence.append(word_features(sentence, i))\n",
    "                y_sentence.append(sentence[i][1])\n",
    "            X_train.append(X_sentence)\n",
    "            y_train.append(y_sentence) \n",
    "\n",
    "\n",
    "        X_test = []\n",
    "        y_test = []\n",
    "        for sentence in corrupt_test_set:\n",
    "            X_sentence = []\n",
    "            y_sentence = []\n",
    "            for i in range(len(sentence)):\n",
    "                X_sentence.append(word_features(sentence, i))\n",
    "                y_sentence.append(sentence[i][1])\n",
    "            X_test.append(X_sentence)\n",
    "            y_test.append(y_sentence) \n",
    "        \n",
    "        \n",
    "        MEMM_train = []  # Collect feature-label pairs for MEMM\n",
    "        for sentence_features, sentence_labels in zip(X_train, y_train):\n",
    "            MEMM_train.extend(list(zip(sentence_features, sentence_labels)))\n",
    "\n",
    "        MEMM_test = []  # Collect feature-label pairs for MEMM\n",
    "        for sentence_features, sentence_labels in zip(X_test, y_test):\n",
    "            MEMM_test.extend(list(zip(sentence_features, sentence_labels)))\n",
    "\n",
    "        \n",
    "        # training using the tuned value\n",
    "        maxent_classifier = MaxentClassifier.train(MEMM_train, algorithm='gis', max_iter=max_iters)\n",
    "\n",
    "        # predictions \n",
    "        memm_predictions = maxent_classifier.classify_many([features for features, _ in MEMM_test])\n",
    "\n",
    "        memm_true_labels = [pos for _, pos in MEMM_test]\n",
    "        \n",
    "        \n",
    "        f1 = f1_score(memm_true_labels, memm_predictions, average='weighted')\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "\n",
    "    return f1_scores\n",
    "\n",
    "\n",
    "#train_and_evaluate_memm(Xdata=X, ydata=y, num_repetitions=2, train_prop=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CRF function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_and_evaluate_crf(data, num_repetitions, prob, param_grid):\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Train and evaluate a CRF tagger.\n",
    "\n",
    "    Parameters:\n",
    "    - data: List of feature-label pairs.\n",
    "    - num_repetitions: number of times to repeat the experiment.\n",
    "    - prob: probability of randomly switching letters, passed to the modify_sentences function\n",
    "    - param_grid: dictionary of parameter specifications for the maximum number of iterations and regularization parameters:\n",
    "\n",
    "\n",
    "    Returns:\n",
    "    - f1_scores: list of f1 scores with length equal to num_repetitions.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # initialize list to keep track of model performance\n",
    "    f1_scores = []\n",
    "\n",
    "    for _ in range(num_repetitions):\n",
    "\n",
    "         # train test split and formatting\n",
    "        train_set, test_set = train_test_split(data, train_size=0.8)\n",
    "\n",
    "        \n",
    "        # corrupt test set\n",
    "        corrupt_test_set = modify_sentences(test_set, probability=prob)\n",
    "\n",
    "        # feature extraction    \n",
    "        X_train = []\n",
    "        y_train = []\n",
    "        for sentence in train_set:\n",
    "            X_sentence = []\n",
    "            y_sentence = []\n",
    "            for i in range(len(sentence)):\n",
    "                X_sentence.append(word_features(sentence, i))\n",
    "                y_sentence.append(sentence[i][1])\n",
    "            X_train.append(X_sentence)\n",
    "            y_train.append(y_sentence) \n",
    "\n",
    "\n",
    "        X_test = []\n",
    "        y_test = []\n",
    "        for sentence in corrupt_test_set:\n",
    "            X_sentence = []\n",
    "            y_sentence = []\n",
    "            for i in range(len(sentence)):\n",
    "                X_sentence.append(word_features(sentence, i))\n",
    "                y_sentence.append(sentence[i][1])\n",
    "            X_test.append(X_sentence)\n",
    "            y_test.append(y_sentence)    \n",
    "   \n",
    "\n",
    "        \n",
    "\n",
    "        # training using the tuned value\n",
    "        trainer = pycrfsuite.Trainer(verbose=False)\n",
    "\n",
    "        # Add training data\n",
    "        for x, y in zip(X_train, y_train):\n",
    "\t        trainer.append(x, y)\n",
    "\n",
    "        # Set trainer parameters\n",
    "        trainer.set_params(param_grid)\n",
    "\n",
    "        # Train the CRF model\n",
    "        trainer.train('pos.crfsuite')\n",
    "\n",
    "\n",
    "        # Testing\n",
    "        # Initialize the tagger\n",
    "        tagger = pycrfsuite.Tagger()\n",
    "        tagger.open('pos.crfsuite')\n",
    "\n",
    "\n",
    "        # predictions\n",
    "        CRF_predictions = [tagger.tag(instance) for instance in X_test]\n",
    "\n",
    "        CRF_flat_predictions = [tag for instance_tags in CRF_predictions for tag in instance_tags]\n",
    "        CRF_flat_ground_truth = [tag for instance_tags in y_test for tag in instance_tags]\n",
    "\n",
    "        # test score\n",
    "        f1 = f1_score(CRF_flat_ground_truth, CRF_flat_predictions, average='weighted')\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "\n",
    "    return f1_scores\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collecting all results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random.seed(1234)\n",
    "\n",
    "# unigram_clean = train_and_evaluate_unigram(data=model_set, num_repetitions=5, prob=0)\n",
    "# unigram_low = train_and_evaluate_unigram(data=model_set, num_repetitions=5, prob=0.05)\n",
    "# unigram_mid = train_and_evaluate_unigram(data=model_set, num_repetitions=5, prob=0.2)\n",
    "# unigram_high = train_and_evaluate_unigram(data=model_set, num_repetitions=5, prob=0.35)\n",
    "\n",
    "# brill_clean = train_and_evaluate_brill(data=model_set, num_repetitions=5, prob=0)\n",
    "# brill_low = train_and_evaluate_brill(data=model_set, num_repetitions=5, prob=0.05)\n",
    "# brill_mid = train_and_evaluate_brill(data=model_set, num_repetitions=5, prob=0.2)\n",
    "# brill_high = train_and_evaluate_brill(data=model_set, num_repetitions=5, prob=0.35)\n",
    "\n",
    "\n",
    "# hmm_clean = train_and_evaluate_hmm(data=model_set, num_repetitions=5, prob=0)\n",
    "# hmm_low = train_and_evaluate_hmm(data=model_set, num_repetitions=5, prob=0.05)\n",
    "# hmm_mid = train_and_evaluate_hmm(data=model_set, num_repetitions=5, prob=0.2)\n",
    "# hmm_high = train_and_evaluate_hmm(data=model_set, num_repetitions=5, prob=0.35)\n",
    "\n",
    "# memm_clean = train_and_evaluate_memm(data=model_set, num_repetitions=5, prob=0, max_iters=best_max_iter)\n",
    "# memm_low = train_and_evaluate_memm(data=model_set, num_repetitions=5, prob=0.05, max_iters=best_max_iter)\n",
    "# memm_mid = train_and_evaluate_memm(data=model_set, num_repetitions=5, prob=0.2, max_iters=best_max_iter)\n",
    "# memm_high = train_and_evaluate_memm(data=model_set, num_repetitions=5, prob=0.35, max_iters=best_max_iter)\n",
    "\n",
    "# crf_clean = train_and_evaluate_crf(data=model_set, num_repetitions=5, prob=0, param_grid=best_param)\n",
    "# crf_low = train_and_evaluate_crf(data=model_set, num_repetitions=5, prob=0.05, param_grid=best_param)\n",
    "# crf_mid = train_and_evaluate_crf(data=model_set, num_repetitions=5, prob=0.2, param_grid=best_param)\n",
    "# crf_high = train_and_evaluate_crf(data=model_set, num_repetitions=5, prob=0.35, param_grid=best_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1_scores = {\n",
    "#     \"hmm_clean\": hmm_clean,\n",
    "#     \"hmm_low\": hmm_low,\n",
    "#     \"hmm_mid\": hmm_mid,\n",
    "#     \"hmm_high\": hmm_high,\n",
    "#     \"memm_clean\": memm_clean,\n",
    "#     \"memm_low\": memm_low,\n",
    "#     \"memm_mid\": memm_mid,\n",
    "#     \"memm_high\": memm_high,\n",
    "#     \"crf_clean\": crf_clean,\n",
    "#     \"crf_low\": crf_low,\n",
    "#     \"crf_mid\": crf_mid,\n",
    "#     \"crf_high\": crf_high,\n",
    "#     \"unigram_clean\": unigram_clean,\n",
    "#     \"unigram_low\": unigram_low,\n",
    "#     \"unigram_mid\": unigram_mid,\n",
    "#     \"unigram_high\": unigram_high,\n",
    "#     \"brill_clean\": brill_clean,\n",
    "#     \"brill_low\": brill_low,\n",
    "#     \"brill_mid\": brill_mid,\n",
    "#     \"brill_high\": brill_high\n",
    "# }\n",
    "\n",
    "# with open('insert file path here', \"w\") as file:\n",
    "#     for model_threshold, f1_score in f1_scores.items():\n",
    "#         file.write(f\"{model_threshold}: {f1_score}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('insert file path here', \"w\") as file:\n",
    "#     for param, paramval in best_param.items():\n",
    "#         file.write(f\"{param}: {paramval}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_it = {\n",
    "#     \"max_it\": best_max_iter\n",
    "# }\n",
    "\n",
    "# with open('insert file path here', \"w\") as file:\n",
    "#     for param, paramval in max_it.items():\n",
    "#         file.write(f\"{param}: {paramval}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
